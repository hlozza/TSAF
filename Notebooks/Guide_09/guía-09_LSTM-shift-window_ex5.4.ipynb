{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Guía 9 - Pronósticos de demanda diaria con LSTM usando Keras y TensorFlow\n",
    "\n",
    "## Pronósticos a partir de la serie temporal de la demanda diaria incorporando variables exógenas desde Datasets\n",
    "\n",
    "La propuesta es dar pronósticos de la demanda diaria de energía eléctrica a partir de sus valores registrados en días anteriores y de otros atributos que podrían tener influencia (variables exógenas) alamcenados en `tf.data.Dataset`. \n",
    "\n",
    "Continuaremos con nuestro ejemplo tomando la base de datos de demanda diaria de energía eléctrica publicados por CAMMESA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Inicio de la programación para el análisis y pronóstico de la serie temporal\n",
    "\n",
    "Comenzamos importando las bibliotecas necesarias y confirmamos la versión de TensorFlow (TF) disponible en la instalación. Esto nos puede ayudar a interpretar algunas diferencias en el comportamiento de las funciones de TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(99)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Carga del conjunto de datos para su análisis\n",
    "\n",
    "Cargamos los datos de demanda diaria. DEMANDA TOTAL es la serie que queremos predecir (target). Esperamos que sus valores históricos nos provean información para su pronóstico. Además, tomaremos otros atributos que consideramos relevantes como `TEMPERATURA REFERENCIA MEDIA GBA °C` o `Tipo día` que nos informa si la jornada fue laborable, semilaborable o no laborable. Las columnas que se refieren a los registros de demanda para otras regiones argentinas no las consideraremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "dataFrame = pd.read_excel('Data/Base Demanda Diaria 2017 2024.xlsx', sheet_name='Datos Región', skiprows=4)  \n",
    "dataFrame.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Separación de los conjuntos de datos de entrenamiento, validación y prueba\n",
    "\n",
    "Como es habitual, separamos el conjunto de datos en entrenamiento y prueba. Como sabemos, se reserva una porción del final de la serie temporal como datos de prueba. En este ejercicio, desdoblamos en datos numéricos y categóricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data = {}\n",
    "\n",
    "col_names = {'DEMANDA TOTAL': 'num0_input', 'TEMPERATURA REFERENCIA MEDIA GBA °C': 'num1_input', 'Tipo día': 'cat_input'}\n",
    "new_df = dataFrame.rename(columns=col_names)\n",
    "\n",
    "for key in col_names.values():\n",
    "    if new_df[key].dtype == np.float64:\n",
    "        orig_data[key] = np.float32(new_df[key].to_numpy())\n",
    "    else:\n",
    "        orig_data[key] = new_df[key].to_numpy()\n",
    "\n",
    "orig_ds = tf.data.Dataset.from_tensor_slices(orig_data)\n",
    "\n",
    "orig_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in orig_ds.take(1):\n",
    "    print(element['num0_input'].shape)\n",
    "    print(element['num0_input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Alternativas para time series windows\n",
    "\n",
    "usando `.batch()` y `.window()`\n",
    "\n",
    "TF: Build TensorFlow input pipelines\n",
    "\n",
    "[Time series windowing](https://www.tensorflow.org/guide/data#time_series_windowing)\n",
    "\n",
    "## Ventanas de datos\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 28\n",
    "OUT_LENGTH = 1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "windowed_ds = orig_ds.window(size=SEQ_LENGTH + OUT_LENGTH, shift=1, drop_remainder=True)\n",
    "\n",
    "# Applying window to a Dataset of dictionaries gives a dictionary of Datasets\n",
    "#for windows in windowed_ds.take(1):\n",
    "#  print(tf.nest.map_structure(to_numpy, windows))\n",
    "\n",
    "new_ds = windowed_ds.flat_map(lambda window: tf.data.Dataset.zip(\n",
    "    dict([(k, v.batch(SEQ_LENGTH + OUT_LENGTH)) for k, v in window.items()])))\n",
    "\n",
    "dict_ds = new_ds.map(lambda x: ((x['num0_input'][:-OUT_LENGTH], x['num1_input'][:-OUT_LENGTH],\n",
    "                                 x['cat_input'][:-OUT_LENGTH]), x['num0_input'][-OUT_LENGTH:]))\n",
    "\n",
    "print(dict_ds.element_spec, '\\n')\n",
    "dict_ds = dict_ds.map(lambda x,y: ((tf.stack([x[0], x[1]], axis=1), tf.expand_dims(x[2], 1)), y))\n",
    "\n",
    "\n",
    "dict_ds = dict_ds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n",
    "print(dict_ds.element_spec, '\\n')\n",
    "for a,b in dict_ds.take(1):\n",
    "    print(a[0].shape, a[1].shape, b.shape)\n",
    "#    print(a[1][1,:,:])\n",
    "#    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Preguntamos si conocemos la cantidad de lotes que conforman el `DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = dict_ds.cardinality()\n",
    "print((DATASET_SIZE == tf.data.UNKNOWN_CARDINALITY).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Como no se sabe la cantidad de lotes que forman el `DATASET`, entonces cargamos manualmente su número mediante un cálculo que considera el tamaño original y el de los lotes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = (new_df.shape[0] - SEQ_LENGTH - OUT_LENGTH) // BATCH_SIZE\n",
    "dict_ds = dict_ds.apply(tf.data.experimental.assert_cardinality(DATASET_SIZE))\n",
    "print(dict_ds.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Separamos en datos de entrenamiento, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.82 * DATASET_SIZE)\n",
    "val_size = int(0.1 * DATASET_SIZE)\n",
    "\n",
    "train_ds = dict_ds.take(train_size)\n",
    "val_ds = dict_ds.skip(train_size).take(val_size)\n",
    "test_ds = dict_ds.skip(train_size).skip(val_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Ahora no es necesario separar en atributos (`X`) y targets (`y`), solo debemos pasar los `Datasets` de entrenamiento o prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Adaptar para normalizar los atributos numéricos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numeric features\n",
    "# normalize0_layer = keras.layers.Normalization(axis=0, mean=15208, variance=4289041)\n",
    "normalize_layer = keras.layers.Normalization(axis=2)\n",
    "normalize_layer.adapt(train_ds.map(lambda X, y: X[0]))\n",
    "normalize_layer.get_config()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Podemos conocer la media y la varianza que luego podríamos usar para invertir los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalize_layer.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DEMANDA TOTAL - Mean: {0:.0f}, Std.Dev {1:.0f}'.format(normalize_layer.mean[0,0,0],\n",
    "                                                           np.sqrt(normalize_layer.variance[0,0,0])))\n",
    "print('Temperatura - Mean: {0:.1f}, Std.Dev. {1:.1f}'.format(normalize_layer.mean[0,0,1],\n",
    "                                                             np.sqrt(normalize_layer.variance[0,0,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Agregando la capa custom de preprocesado para los atributos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our definition for the categorical preporcossing layer\n",
    "class OneHotPre(keras.layers.Layer):\n",
    "    def __init__(self, max_tokens, vocabulary=None, num_oov_indices=0, **kwargs):\n",
    "        self._lower_lyr = tf.keras.layers.Lambda(lambda x: tf.strings.lower(x))\n",
    "        self._lab_lyr = tf.keras.layers.Lambda(lambda x: \n",
    "                                           tf.strings.regex_replace(x, '^.*h.bil.*$', 'laborable'))\n",
    "        self._semilab_lyr = tf.keras.layers.Lambda(lambda x:\n",
    "                                                   tf.strings.regex_replace(x,'^.*semilaborable.*$', 'semilaborable'))\n",
    "        self._nolab_lyr = tf.keras.layers.Lambda(lambda x:\n",
    "                                                 tf.strings.regex_replace(x,'^.*feriado.*$', 'nolaborable'))\n",
    "        self._lookup_layer = tf.keras.layers.StringLookup(\n",
    "            max_tokens = max_tokens,\n",
    "            num_oov_indices = num_oov_indices,\n",
    "            vocabulary = vocabulary,\n",
    "            # output_mode = 'int')\n",
    "            output_mode = 'one_hot')\n",
    "        super().__init__(**kwargs)\n",
    "    def _to_day_type(self, value):\n",
    "        day_type = self._lower_lyr(value)\n",
    "        day_type = self._lab_lyr(day_type)\n",
    "        day_type = self._semilab_lyr(day_type)\n",
    "        return self._nolab_lyr(day_type)\n",
    "    def adapt(self, data_sample):\n",
    "        map_ds = data_sample.map(lambda x: self._lower_lyr(x))\n",
    "        map_ds = map_ds.map(lambda x: self._lab_lyr(x))\n",
    "        map_ds = map_ds.map(lambda x: self._semilab_lyr(x))\n",
    "        map_ds = map_ds.map(lambda x: self._nolab_lyr(x))\n",
    "        self._lookup_layer.adapt(map_ds)\n",
    "        # self._lookup_layer.adapt(data_sample)\n",
    "    def call(self, inputs):\n",
    "        # return self._lookup_layer(self._to_day_type(inputs))\n",
    "        ll = self._lower_lyr(inputs)\n",
    "        ll = self._lab_lyr(ll)\n",
    "        ll = self._semilab_lyr(ll)\n",
    "        ll = self._nolab_lyr(ll)\n",
    "        return self._lookup_layer(ll)\n",
    "    def get_config(self):\n",
    "        return {'vocabulary': self._lookup_layer.get_vocabulary()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Adaptando la capa custom para preprocesar los attributos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "one_hot_layer = OneHotPre(3)\n",
    "one_hot_layer.adapt(train_ds.map(lambda X, y: X[1]))\n",
    "one_hot_layer.get_config()\n",
    "\n",
    "#one_hot_layer = OneHotPre(index, 7, ['Domingo o Feriado',\n",
    "#                                     'Sabado o Semilaborable', \n",
    "#                                     'Miercoles habiles', \n",
    "#                                     'Jueves Habiles', \n",
    "#                                     'Martes Habiles',\n",
    "#                                     'Viernes habiles',\n",
    "#                                     'Lunes habiles'])\n",
    "one_hot_layer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Confirmamos los tamaños de los nuevos conjuntos de datos particionados en lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    inputs, outputs = batch\n",
    "    # outputs = outputs[...,tf.newaxis]\n",
    "    print('Input len: ', len(inputs), 'Output len:', len(outputs))\n",
    "    print('Input 0 (num) shape: ', inputs[0].shape)\n",
    "    print('Input 1 (cat) shape: ', inputs[1].shape)\n",
    "    print('Ouput shape: ', outputs.shape)\n",
    "    other0=normalize_layer(inputs[0])\n",
    "    other2=one_hot_layer(inputs[1])\n",
    "    print('Normalized shape ', other0.shape)\n",
    "    print('One-hot shape ', other2.shape)\n",
    "    print('One-hot type ', other2.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Definición del modelo \n",
    "\n",
    "\n",
    "\n",
    "En este caso, definimos el modelo de manera que procese la información de entrada por 2 canales diferentes. Los datos numéricos se normalizan, los categóricos pasan por la capa custom de codificación one-hot. Luego, ambas salidas se unen para continuar su procesamiento en la primer capa con celdas LSTM. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Como pasar multiples datos de entrada desde un Dataset\n",
    "\n",
    "TF: Training & evaluation with the built-in methods \n",
    "\n",
    "the Dataset should return a tuple of dicts.\n",
    "\n",
    "[Passing data to multi-input, multi-output models](https://www.tensorflow.org/guide/keras/training_with_built_in_methods#passing_data_to_multi-input_multi-output_models)\n",
    "\n",
    "y también ver TF: The Functional API\n",
    "\n",
    "When calling fit with a Dataset object, it should yield either a tuple of lists like `([title_data, body_data, tags_data], [priority_targets, dept_targets])` or a tuple of dictionaries like `({'title': title_data, 'body': body_data, 'tags': tags_data}, {'priority': priority_targets, 'department': dept_targets})`.\n",
    "\n",
    "[Manipulate complex graph topologies](https://www.tensorflow.org/guide/keras/functional_api#manipulate_complex_graph_topologies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalize_layer = keras.layers.Normalization(axis=1, mean=normalize_layer.mean[0,0,0],\n",
    "                                              variance=normalize_layer.variance[0,0,0],\n",
    "                                               invert=True, name='output')\n",
    "\n",
    "# Functional API Model \n",
    "numeric_input = keras.layers.Input(shape=(SEQ_LENGTH,2), dtype=tf.float32, name='num_input')\n",
    "normalized = normalize_layer(numeric_input)\n",
    "categorical_input = keras.layers.Input(shape=(SEQ_LENGTH,1), dtype=tf.string, name='cat_input')\n",
    "encoded = one_hot_layer(categorical_input)\n",
    "reshape_layer = tf.keras.layers.Reshape((SEQ_LENGTH,3))(encoded)\n",
    "concat = keras.layers.concatenate([normalized, reshape_layer])\n",
    "lstm_lyr = keras.layers.LSTM(10)(concat)\n",
    "dropout_lyr = keras.layers.Dropout(0.2)(lstm_lyr)\n",
    "dense_lyr = keras.layers.Dense(1, activation='linear')(dropout_lyr)\n",
    "unnormalized = unnormalize_layer(dense_lyr)\n",
    "model_exo = keras.models.Model(inputs=[numeric_input, categorical_input], outputs=[unnormalized])\n",
    "model_exo.compile(loss = 'MeanSquaredError', metrics=['MAE'], optimizer='Adam')\n",
    "# model_exo.compile(loss = {'output': tf.keras.losses.MeanSquaredError()},\n",
    "#                  metrics={'output': tf.keras.metrics.MeanAbsoluteError()},\n",
    "#                  optimizer='Adam')\n",
    "model_exo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "El método `.summary()` nos muestra cómo fluirá la información entre las capas de NN y cuántos parámetros se deberán ajustar en cada una. Otra alternativa para representar la configuración de la NN es usar `tf.keras.utils.plot_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model_exo, to_file='Figs/model.png', rankdir=\"TB\", show_shapes=True,  show_layer_names=True)\n",
    "\n",
    "#tf.keras.utils.plot_model(multivariate_lstm, to_file='Figs/model.png', show_shapes=True,\n",
    "#                          show_dtype=True, show_layer_names=True, rankdir='TB',\n",
    "#                          expand_nested=True, dpi=200, show_layer_activations=True,\n",
    "#                          show_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Ajuste del modelo\n",
    "\n",
    "Entrenamos el modelo, es decir, ajustamos los pesos de acuerdo al esquema definido cuando fue compilado, por ejemplo optimizer='Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "history = model_exo.fit(train_ds, epochs=30, validation_data=val_ds)\n",
    "\n",
    "# early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "# history = model_exo.fit(train_ds, epochs=3000, validation_data=val_ds, callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Para tener una idea sobre la convergencia del moelo, mostramos las curvas de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "# plt.gca().set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Luego que el modelo ha sido ajustado, es muy fácil obtener nuevas predicciones, como por ejemplo cuando le pasamos el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict discards targets\n",
    "predicted_values = model_exo.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_values = test_ds.map(lambda X, y: y['output'])\n",
    "#target_values = target_values.flat_map(tf.data.Dataset.from_tensor_slices)\n",
    "#print(type(target_values))\n",
    "#target_values = np.fromiter(target_values.as_numpy_iterator(), dtype=np.float32)\n",
    "#print(target_values.shape)\n",
    "\n",
    "# or\n",
    "target_values = test_ds.map(lambda X, y: y)\n",
    "target_values = target_values.unbatch()\n",
    "print(type(target_values))\n",
    "target_values = list(target_values.as_numpy_iterator())\n",
    "print(len(target_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Evaluación de los resultados\n",
    "\n",
    "Ahora vamos a representar nuestro resultado. Construimos un `DataFrame` para almacenar los valores de `DEMANDA TOTAL` predecida y observada, y su fecha (que recuperamos de los datos originales). Este `DataFrame` nos facilitará el cálculo de distintas métricas que califican el grado de acuerdo de las predicciones del modelo con los valores registrados. Asimismo, representaremos en una figura ambas series de datos para tener una estimación visual del desempeño del modelo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast Plot with Dates on X-axis\n",
    "LSTM_eval = pd.DataFrame({\n",
    "    'Predicted_DEMANDA': predicted_values[:, 0],\n",
    "    'Actual_DEMANDA': target_values,\n",
    "})\n",
    "\n",
    "LSTM_eval.loc[:, 'Date'] = dataFrame['Fecha'][-len(target_values):].values\n",
    "# LSTM_eval.set_index('Date', inplace=True)\n",
    "LSTM_eval['Date'] = pd.to_datetime(LSTM_eval['Date'])\n",
    "LSTM_eval.set_index('Date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una figura para representar las series de datos\n",
    "def FigPredActual(d, title):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    #  highlight the  forecast\n",
    "    # highlight_start = int(len(d) * 0.9)  \n",
    "    # highlight_end = len(d) - 1  # Adjusted to stay within bounds\n",
    "    # Plot the actual values\n",
    "    # plt.plot(d[['Actual_DEMANDA']][:highlight_start], label=['Actual_DEMANDA'])\n",
    "    plt.plot(d[['Actual_DEMANDA']], label=['Actual_DEMANDA'])\n",
    "    \n",
    "    # Plot predicted values with a dashed line\n",
    "    plt.plot(d[['Predicted_DEMANDA']], label=['Predicted_DEMANDA'], linestyle='--')\n",
    "    \n",
    "    # Highlight the forecasted portion with a different color\n",
    "    # plt.axvspan(d.index[highlight_start], d.index[highlight_end], facecolor='lightgreen', alpha=0.5, label='Forecast')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Dates')\n",
    "    plt.ylabel('Values')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Trazamos ambas series de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigPredActual(LSTM_eval, 'Multivariate Time-Series forecasting using LSTM and Exogenous variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Definimos una función conveniente para calcular diferentes métrias que nos informan sobre el grado de acuerdo de las predicciones del modelo con las observaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def performance(d):\n",
    "    return {\n",
    "        'MSE': mean_squared_error(d['Actual_DEMANDA'].to_numpy(), d['Predicted_DEMANDA'].to_numpy()),\n",
    "        'MAE': mean_absolute_error(d['Actual_DEMANDA'].to_numpy(), d['Predicted_DEMANDA'].to_numpy()),\n",
    "        'R2': r2_score(d['Actual_DEMANDA'].to_numpy(), d['Predicted_DEMANDA'].to_numpy())\n",
    "    }\n",
    "\n",
    "performance(LSTM_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "**Preguntas**\n",
    "\n",
    "1. ¿Mejora el ajuste si `BATCH_SIZE` y `SEQ_LENGTH` coinciden?\n",
    "2. ¿Es importante que `SEQ_LENGTH` tome una cantidad de días que corresponda a un número entero de semanas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Ajustar la serie temporal de `DEMANDA TOTAL` usando solamente la variable endógena construtendo los `DATASETS` de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(dataFrame['DEMANDA TOTAL'])\n",
    "dataset = dataset.window(SEQ_LENGTH + OUT_LENGTH, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(SEQ_LENGTH + OUT_LENGTH))\n",
    "dataset = dataset.map(lambda window: (window[:-OUT_LENGTH], window[-OUT_LENGTH:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "¿Se podría repetir esta técnica sobre otros atributos para incluir variables exógenas, como una alternativa al uso de diccionarios?\n",
    "\n",
    "Para más información ver también:\n",
    "\n",
    "* [How to use windows created by the Dataset.window() method in TensorFlow 2.0?](https://stackoverflow.com/questions/55429307/how-to-use-windows-created-by-the-dataset-window-method-in-tensorflow-2-0)\n",
    "* tf.data: Build TensorFlow input pipelines [Time series windowing](https://www.tensorflow.org/guide/data#time_series_windowing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "2. Ajustar la serie temporal de `DEMANDA TOTAL` usando solamente la variable endógena construyendo los `DATASETS` de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data, target_idx=0, out_len=28, ts_num=28):\n",
    "    input_data = data[:-ts_num]\n",
    "    if target_idx is not None:\n",
    "        targets = data[ts_num:, target_idx]\n",
    "        targets = targets[...,np.newaxis]\n",
    "    else:\n",
    "        targets = None\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=input_data,\n",
    "        targets=targets,\n",
    "        sequence_length=out_len, #Length of the output sequences (in number of timesteps). \n",
    "        sequence_stride=1, #Period between successive output sequences\n",
    "        shuffle=False,\n",
    "        batch_size=ts_num, #Number of timeseries samples in each batch\n",
    "        seed=43)\n",
    "    return ds\n",
    "\n",
    "# num_attr_ds = make_dataset(dataFrame['DEMANDA TOTAL'], 0, SEQ_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "`tf.keras.preprocessing.timeseries_dataset_from_array` toma una secuencia de datos reunidos en intervalos iguales, junto con parámetros de la serie temporal como longitud de la secuencia/ventana, espaciamiento de la secuencia/ventana, etc., para producir lotes de series temporales de datos de entrada y de de targets.\n",
    "\n",
    "¿Se podría repetir esta técnica sobre otros atributos para incluir variables exógenas, como una alternativa al uso de diccionarios?\n",
    "\n",
    "Para más información ver también:\n",
    "* [timeseries_dataset_from_array](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array)\n",
    "* Un guía completa sobre como armar distintas ventanas de datos usando `timeseries_dataset_from_array` se enecuentra en [Time series forecasting](https://www.tensorflow.org/tutorials/structured_data/time_series). El enlace es una referencia al uso de NN para pronósticos de series temporales \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.12-TF2.18",
   "language": "python",
   "name": "venv-tf2.18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
