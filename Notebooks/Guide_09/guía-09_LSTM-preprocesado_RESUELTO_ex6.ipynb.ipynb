{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Pronósticos de demanda diaria con LSTM usando Keras y TensorFlow\n",
    "\n",
    "## Pronósticos a partir de la serie temporal de la demanda diaria incorporando variables exógenas desde Datasets\n",
    "\n",
    "**Este es básicamente idéntico a StringLookup_5 salvo que se anticipa el preprocesamiento de los datos categóricos y se quita la cap `OneHotPre` del modelo formando el `Dataset` ya con las columnas tipo one-hot numéricas (antes al modelo llegaba un solo atributo pero del tipo `tf.string`**\n",
    "\n",
    "La propuesta es dar pronósticos de la demanda diaria de energía eléctrica a partir de sus valores registrados en días anteriores y de otros atributos que podrían tener influencia (variables exógenas) alamcenados en `tf.data.Dataset`. \n",
    "\n",
    "Continuaremos con nuestro ejemplo tomando la base de datos de demanda diaria de energía eléctrica publicados por CAMMESA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Inicio de la programación para el análisis y pronóstico de la serie temporal\n",
    "\n",
    "Comenzamos importando las bibliotecas necesarias y confirmamos la versión de TensorFlow (TF) disponible en la instalación. Esto nos puede ayudar a interpretar algunas diferencias en el comportamiento de las funciones de TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(99)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Carga del conjunto de datos para su análisis\n",
    "\n",
    "Cargamos los datos de demanda diaria. DEMANDA TOTAL es la serie que queremos predecir (target). Esperamos que sus valores históricos nos provean información para su pronóstico. Además, tomaremos otros atributos que consideramos relevantes como `TEMPERATURA REFERENCIA MEDIA GBA °C` o `Tipo día` que nos informa si la jornada fue laborable, semilaborable o no laborable. Las columnas que se refieren a los registros de demanda para otras regiones argentinas no las consideraremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "dataFrame = pd.read_excel('Data/Base Demanda Diaria 2017 2024.xlsx', sheet_name='Datos Región', skiprows=4)  \n",
    "dataFrame.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Separación de los conjuntos de datos de entrenamiento, validación y prueba\n",
    "\n",
    "Como es habitual, separamos el conjunto de datos en entrenamiento y prueba. Como sabemos, se reserva una porción del final de la serie temporal como datos de prueba. En este ejercicio, desdoblamos en datos numéricos y categóricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TensorFlow tensors require that all elements have the same dtype.\n",
    "numeric_feature_names = ['DEMANDA TOTAL', 'TEMPERATURA REFERENCIA MEDIA GBA °C']\n",
    "column_indices = {name: i for i, name in enumerate(numeric_feature_names)}\n",
    "numeric_tensor = np.float32(dataFrame[numeric_feature_names].to_numpy())\n",
    "# cast to tf defaults... float32\n",
    "\n",
    "categoric_feature_names = ['Tipo día']\n",
    "categoric_tensor = dataFrame[categoric_feature_names].to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "En lo siguiente utilizaremos `tf.keras.preprocessing.timeseries_dataset_from_array` que crea conjuntos de datos de ventanas móviles sobre series temporales provistas como arrays. Por esto mantenemos en el paso anterior la transformación `.to_numpy()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Alternativas para time series windows\n",
    "\n",
    "usando `.batch()` y `.window()`\n",
    "\n",
    "TF: Build TensorFlow input pipelines\n",
    "\n",
    "[Time series windowing](https://www.tensorflow.org/guide/data#time_series_windowing)\n",
    "\n",
    "## Ventanas de datos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Necesitamos particionar la serie temporal en lotes operables por las sucesivas capas de NN. En este ejemplo tomaremos funciones desarrolladas específicamente para operar con series temporales en lugar de definir nuestras propias funciones que realizan los cortes en lotes de datos.\n",
    "\n",
    "Como lo adelantamos, `tf.keras.preprocessing.timeseries_dataset_from_array` toma una secuencia de datos reunidos en intervalos iguales, junto con parámetros de la serie temporal como longitud de la secuencia/ventana, espaciamiento de la secuencia/ventana, etc., para producir lotes de series temporales de datos de entrada y de de targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dataset(data, target_idx=0, out_len=28, ts_num=28):\n",
    "    input_data = data[:-ts_num]\n",
    "    if target_idx is not None:\n",
    "        targets = data[ts_num:, target_idx]\n",
    "        targets = targets[...,np.newaxis]\n",
    "    else:\n",
    "        targets = None\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=input_data,\n",
    "        targets=targets,\n",
    "        sequence_length=out_len, #Length of the output sequences (in number of timesteps). \n",
    "        sequence_stride=1, #Period between successive output sequences\n",
    "        shuffle=False,\n",
    "        batch_size=ts_num, #Number of timeseries samples in each batch\n",
    "        seed=43)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=28\n",
    "SEQ_LENGTH=28\n",
    "num_attr_ds = make_dataset(numeric_tensor, 0, SEQ_LENGTH, BATCH_SIZE)\n",
    "num_attr_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Creamos un `Dataset` con los datos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cattr_ds = make_dataset(categoric_tensor, None, SEQ_LENGTH, BATCH_SIZE)\n",
    "cattr_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cattr_ds = cattr_ds.map(lambda X: tf.strings.lower(X))\n",
    "cattr_ds = cattr_ds.map(lambda X: tf.strings.regex_replace(X, '^.*h.bil.*$', 'laborable'))\n",
    "cattr_ds = cattr_ds.map(lambda X: tf.strings.regex_replace(X, '^.*semilaborable.*$', 'semilaborable'))\n",
    "cattr_ds = cattr_ds.map(lambda X: tf.strings.regex_replace(X, '^.*feriado.*$', 'nolaborable'))\n",
    "\n",
    "vocabulary = ['laborable', 'semilaborable', 'nolaborable']\n",
    "one_hot_layer = tf.keras.layers.StringLookup(max_tokens = 3,\n",
    "                                            num_oov_indices = 0,\n",
    "                                            vocabulary = vocabulary,\n",
    "                                            output_mode = 'one_hot')\n",
    "one_hot_layer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cattr_ds = cattr_ds.map(lambda X: tf.cast(one_hot_layer(X), tf.float32))\n",
    "\n",
    "for batch in cattr_ds.take(1):\n",
    "    print(batch.shape)\n",
    "    print(batch.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nattr_ds = num_attr_ds.map(lambda X, y: X)\n",
    "tgt_ds = num_attr_ds.map(lambda X, y: y)\n",
    "\n",
    "#dict_ds = tf.data.Dataset.from_tensor_slices(({\"num_input\": nattr_ds}, {\"output\": tgt_ds},))\n",
    "#dict_ds = tf.data.Dataset.from_tensor_slices((nattr_ds, tgt_ds))\n",
    "# dict_ds = tf.data.Dataset.zip((nattr_ds, tgt_ds))\n",
    "dict_ds = tf.data.Dataset.zip({'num_input': nattr_ds, 'cat_input': cattr_ds}, {'output': tgt_ds})\n",
    "DATASET_SIZE = dict_ds.cardinality().numpy()\n",
    "print(DATASET_SIZE)\n",
    "dict_ds.element_spec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.82 * DATASET_SIZE)\n",
    "val_size = int(0.1 * DATASET_SIZE)\n",
    "\n",
    "train_ds = dict_ds.take(train_size)\n",
    "val_ds = dict_ds.skip(train_size).take(val_size)\n",
    "test_ds = dict_ds.skip(train_size).skip(val_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.take(5):\n",
    "    inputs, outputs = batch\n",
    "    # outputs = outputs[...,tf.newaxis]\n",
    "    print(inputs.keys(), outputs.keys())\n",
    "\n",
    "print('Input: ', inputs['num_input'].shape)\n",
    "print('Input: ', inputs['cat_input'].shape)\n",
    "print('Output: ', outputs['output'].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Confirmamos los tamaños de los nuevos conjuntos de datos particionados en lotes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Ahora no es necesario separar en atributos (`X`) y targets (`y`), solo debemos pasar los `Datasets` de entrenamiento o prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Adaptar para normalizar los atributos numéricos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numeric features\n",
    "normalize_layer = keras.layers.Normalization(axis=2)\n",
    "normalize_layer.adapt(train_ds.map(lambda X, y: X['num_input']))\n",
    "normalize_layer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Podemos conocer la media y la varianza que luego podríamos usar para invertir los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DEMANDA TOTAL - Mean: {0:.0f}, Std.Dev {1:.0f}'.format(normalize_layer.mean[0,0,0],\n",
    "                                                           np.sqrt(normalize_layer.variance[0,0,0])))\n",
    "print('Temperatura - Mean: {0:.1f}, Std.Dev. {1:.1f}'.format(normalize_layer.mean[0,0,1],\n",
    "                                                             np.sqrt(normalize_layer.variance[0,0,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Definición del modelo \n",
    "\n",
    "\n",
    "\n",
    "En este caso, definimos el modelo de manera que procese la información de entrada por 2 canales diferentes. Los datos numéricos se normalizan, los categóricos pasan por la capa custom de codificación one-hot. Luego, ambas salidas se unen para continuar su procesamiento en la primer capa con celdas LSTM. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Como pasar multiples datos de entrada desde un Dataset\n",
    "\n",
    "TF: Training & evaluation with the built-in methods \n",
    "\n",
    "the Dataset should return a tuple of dicts.\n",
    "\n",
    "[Passing data to multi-input, multi-output models](https://www.tensorflow.org/guide/keras/training_with_built_in_methods#passing_data_to_multi-input_multi-output_models)\n",
    "\n",
    "y también ver TF: The Functional API\n",
    "\n",
    "When calling fit with a Dataset object, it should yield either a tuple of lists like `([title_data, body_data, tags_data], [priority_targets, dept_targets])` or a tuple of dictionaries like `({'title': title_data, 'body': body_data, 'tags': tags_data}, {'priority': priority_targets, 'department': dept_targets})`.\n",
    "\n",
    "[Manipulate complex graph topologies](https://www.tensorflow.org/guide/keras/functional_api#manipulate_complex_graph_topologies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalize_layer = keras.layers.Normalization(axis=1, mean=normalize_layer.mean[0,0,0],\n",
    "                                              variance=normalize_layer.variance[0,0,0],\n",
    "                                               invert=True, name='output')\n",
    "\n",
    "# Functional API Model \n",
    "numeric_input = keras.layers.Input(shape=(SEQ_LENGTH, 2), dtype=tf.float32, name='num_input')\n",
    "normalized = normalize_layer(numeric_input)\n",
    "categorical_input = keras.layers.Input(shape=(SEQ_LENGTH, 3), dtype=tf.float32, name='cat_input')\n",
    "concat = keras.layers.concatenate([normalized, categorical_input])\n",
    "lstm_lyr = keras.layers.LSTM(10)(concat)\n",
    "dropout_lyr = keras.layers.Dropout(0.2)(lstm_lyr)\n",
    "dense_lyr = keras.layers.Dense(1, activation='linear')(dropout_lyr)\n",
    "unnormalized = unnormalize_layer(dense_lyr)\n",
    "model_exo = keras.models.Model(inputs=[numeric_input, categorical_input], outputs=[unnormalized])\n",
    "model_exo.compile(loss = 'MeanSquaredError', metrics=['MAE'], optimizer='Adam')\n",
    "#model_exo.compile(loss = {'output': tf.keras.losses.MeanSquaredError()},\n",
    "#                  metrics={'output': tf.keras.metrics.MeanAbsoluteError()},\n",
    "#                  optimizer='Adam')\n",
    "model_exo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "El método `.summary()` nos muestra cómo fluirá la información entre las capas de NN y cuántos parámetros se deberán ajustar en cada una. Otra alternativa para representar la configuración de la NN es usar `tf.keras.utils.plot_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model_exo, rankdir=\"LR\", show_shapes=True,  show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Ajuste del modelo\n",
    "\n",
    "Entrenamos el modelo, es decir, ajustamos los pesos de acuerdo al esquema definido cuando fue compilado, por ejemplo optimizer='Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model training\n",
    "history = model_exo.fit(train_ds, epochs=30, validation_data=val_ds)\n",
    "\n",
    "# early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "# history = model_exo.fit(train_ds, epochs=3000, validation_data=val_ds, callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Para tener una idea sobre la convergencia del moelo, mostramos las curvas de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "# plt.gca().set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Luego que el modelo ha sido ajustado, es muy fácil obtener nuevas predicciones, como por ejemplo cuando le pasamos el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict discards targets\n",
    "predicted_values = model_exo.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_values = test_ds.map(lambda X, y: y['output'])\n",
    "#target_values = target_values.flat_map(tf.data.Dataset.from_tensor_slices)\n",
    "#print(type(target_values))\n",
    "#target_values = np.fromiter(target_values.as_numpy_iterator(), dtype=np.float32)\n",
    "#print(target_values.shape)\n",
    "\n",
    "# or\n",
    "target_values = test_ds.map(lambda X, y: y['output'])\n",
    "target_values = target_values.unbatch()\n",
    "print(type(target_values))\n",
    "target_values = list(target_values.as_numpy_iterator())\n",
    "print(len(target_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Evaluación de los resultados\n",
    "\n",
    "Ahora vamos a representar nuestro resultado. Construimos un `DataFrame` para almacenar los valores de `DEMANDA TOTAL` predecida y observada, y su fecha (que recuperamos de los datos originales). Este `DataFrame` nos facilitará el cálculo de distintas métricas que califican el grado de acuerdo de las predicciones del modelo con los valores registrados. Asimismo, representaremos en una figura ambas series de datos para tener una estimación visual del desempeño del modelo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast Plot with Dates on X-axis\n",
    "LSTM_eval = pd.DataFrame({\n",
    "    'Predicted_DEMANDA': predicted_values[:, 0],\n",
    "    'Actual_DEMANDA': target_values,\n",
    "})\n",
    "\n",
    "LSTM_eval.loc[:, 'Date'] = dataFrame['Fecha'][-len(target_values):].values\n",
    "# LSTM_eval.set_index('Date', inplace=True)\n",
    "LSTM_eval['Date'] = pd.to_datetime(LSTM_eval['Date'])\n",
    "LSTM_eval.set_index('Date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una figura para representar las series de datos\n",
    "def FigPredActual(d, title):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    #  highlight the  forecast\n",
    "    # highlight_start = int(len(d) * 0.9)  \n",
    "    # highlight_end = len(d) - 1  # Adjusted to stay within bounds\n",
    "    # Plot the actual values\n",
    "    # plt.plot(d[['Actual_DEMANDA']][:highlight_start], label=['Actual_DEMANDA'])\n",
    "    plt.plot(d[['Actual_DEMANDA']], label=['Actual_DEMANDA'])\n",
    "    \n",
    "    # Plot predicted values with a dashed line\n",
    "    plt.plot(d[['Predicted_DEMANDA']], label=['Predicted_DEMANDA'], linestyle='--')\n",
    "    \n",
    "    # Highlight the forecasted portion with a different color\n",
    "    # plt.axvspan(d.index[highlight_start], d.index[highlight_end], facecolor='lightgreen', alpha=0.5, label='Forecast')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Dates')\n",
    "    plt.ylabel('Values')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Trazamos ambas series de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigPredActual(LSTM_eval, 'Multivariate Time-Series forecasting using LSTM and Exogenous variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Definimos una función conveniente para calcular diferentes métrias que nos informan sobre el grado de acuerdo de las predicciones del modelo con las observaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def performance(d):\n",
    "    return {\n",
    "        'MSE': mean_squared_error(d['Actual_DEMANDA'].to_numpy(), d['Predicted_DEMANDA'].to_numpy()),\n",
    "        'MAE': mean_absolute_error(d['Actual_DEMANDA'].to_numpy(), d['Predicted_DEMANDA'].to_numpy()),\n",
    "        'R2': r2_score(d['Actual_DEMANDA'].to_numpy(), d['Predicted_DEMANDA'].to_numpy())\n",
    "    }\n",
    "\n",
    "performance(LSTM_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "**Preguntas**\n",
    "\n",
    "1. ¿Por qué el ajuste mejora si `BATCH_SIZE` y `SEQ_LENGTH` coinciden (sin importar demasiado si el número de días corresponde a un número entero de semanas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 TF",
   "language": "python",
   "name": "venv-3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
