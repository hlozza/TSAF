{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Guía 9 - Pronósticos de demanda diaria con LSTM usando Keras y TensorFlow\n",
    "\n",
    "## Pronósticos a partir de la serie temporal de la demanda diaria incorporando variables exógenas desde Datasets (make_datsets)\n",
    "\n",
    "La propuesta es dar pronósticos de la demanda diaria de energía eléctrica a partir de sus valores registrados en días anteriores y de otros atributos que podrían tener influencia (variables exógenas) alamcenados en `tf.data.Dataset`. \n",
    "\n",
    "Continuaremos con nuestro ejemplo tomando la base de datos de demanda diaria de energía eléctrica publicados por CAMMESA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Inicio de la programación para el análisis y pronóstico de la serie temporal\n",
    "\n",
    "Comenzamos importando las bibliotecas necesarias y confirmamos la versión de TensorFlow (TF) disponible en la instalación. Esto nos puede ayudar a interpretar algunas diferencias en el comportamiento de las funciones de TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(99)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Carga del conjunto de datos para su análisis\n",
    "\n",
    "Cargamos los datos de demanda diaria. DEMANDA TOTAL es la serie que queremos predecir (target). Esperamos que sus valores históricos nos provean información para su pronóstico. Además, tomaremos otros atributos que consideramos relevantes como `TEMPERATURA REFERENCIA MEDIA GBA °C` o `Tipo día` que nos informa si la jornada fue laborable, semilaborable o no laborable. Las columnas que se refieren a los registros de demanda para otras regiones argentinas no las consideraremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "dataFrame = pd.read_excel('Data/Base Demanda Diaria 2017 2024.xlsx', sheet_name='Datos Región', skiprows=4)  \n",
    "dataFrame.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Separación de los conjuntos de datos de entrenamiento, validación y prueba\n",
    "\n",
    "Como es habitual, separamos el conjunto de datos en entrenamiento y prueba. Como sabemos, se reserva una porción del final de la serie temporal como datos de prueba. En este ejercicio, desdoblamos en datos numéricos y categóricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TensorFlow tensors require that all elements have the same dtype.\n",
    "numeric_feature_names = ['DEMANDA TOTAL', 'TEMPERATURA REFERENCIA MEDIA GBA °C']\n",
    "column_indices = {name: i for i, name in enumerate(numeric_feature_names)}\n",
    "numeric_tensor = np.float32(dataFrame[numeric_feature_names].to_numpy())\n",
    "# cast to tf defaults... float32\n",
    "\n",
    "categoric_feature_names = ['Tipo día']\n",
    "categoric_tensor = dataFrame[categoric_feature_names].to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "En lo siguiente utilizaremos `tf.keras.preprocessing.timeseries_dataset_from_array` que crea conjuntos de datos de ventanas móviles sobre series temporales provistas como arrays. Por esto mantenemos en el paso anterior la transformación `.to_numpy()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Alternativas para time series windows\n",
    "\n",
    "usando `.batch()` y `.window()`\n",
    "\n",
    "TF: Build TensorFlow input pipelines\n",
    "\n",
    "[Time series windowing](https://www.tensorflow.org/guide/data#time_series_windowing)\n",
    "\n",
    "## Ventanas de datos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Necesitamos particionar la serie temporal en lotes operables por las sucesivas capas de NN. En este ejemplo tomaremos funciones desarrolladas específicamente para operar con series temporales en lugar de definir nuestras propias funciones que realizan los cortes en lotes de datos.\n",
    "\n",
    "Como lo adelantamos, `tf.keras.preprocessing.timeseries_dataset_from_array` toma una secuencia de datos reunidos en intervalos iguales, junto con parámetros de la serie temporal como longitud de la secuencia/ventana, espaciamiento de la secuencia/ventana, etc., para producir lotes de series temporales de datos de entrada y de de targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dataset(data, target_idx=0, out_len=28, ts_num=28):\n",
    "    input_data = data[:-ts_num]\n",
    "    if target_idx is not None:\n",
    "        targets = data[ts_num:, target_idx]\n",
    "        targets = targets[...,np.newaxis]\n",
    "    else:\n",
    "        targets = None\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=input_data,\n",
    "        targets=targets,\n",
    "        sequence_length=out_len, #Length of the output sequences (in number of timesteps). \n",
    "        sequence_stride=1, #Period between successive output sequences\n",
    "        shuffle=False,\n",
    "        batch_size=ts_num, #Number of timeseries samples in each batch\n",
    "        seed=43)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Pasamos un 0 a la función `make_dataset` ya que las estiquetas están en la columna 0. Por esto, la función devuelve `Dataset` como una tupla con 2 tensores: el tensor de atributos (2 columnas de atributos, demanda y temperatura media) y el de etiquetas (valor de la demanda desplazada al final de `SEQ_LENGTH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=28\n",
    "SEQ_LENGTH=28\n",
    "num_attr_ds = make_dataset(numeric_tensor, 0, SEQ_LENGTH, BATCH_SIZE)\n",
    "num_attr_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Creamos un `Dataset` con los datos categóricos. Pasamos `None` ya que las etiquetas se definieron en el paso anterior. Recuperamos otro tensor de atributos (en este caso de tipo `tf.string`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cattr_ds = make_dataset(categoric_tensor, None, SEQ_LENGTH, BATCH_SIZE)\n",
    "cattr_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Vamos a formar un diccionario de `Datasets`. Esto permite combinar los distintos tipo de tensores y que sean ingestables por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nattr_ds = num_attr_ds.map(lambda X, y: X)\n",
    "tgt_ds = num_attr_ds.map(lambda X, y: y)\n",
    "\n",
    "#dict_ds = tf.data.Dataset.from_tensor_slices(({\"num_input\": nattr_ds}, {\"output\": tgt_ds},))\n",
    "#dict_ds = tf.data.Dataset.from_tensor_slices((nattr_ds, tgt_ds))\n",
    "# dict_ds = tf.data.Dataset.zip((nattr_ds, tgt_ds))\n",
    "dict_ds = tf.data.Dataset.zip((nattr_ds, cattr_ds), (tgt_ds))\n",
    "DATASET_SIZE = dict_ds.cardinality().numpy()\n",
    "print(DATASET_SIZE)\n",
    "dict_ds.element_spec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Separamos en datos de entrenamiento, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.82 * DATASET_SIZE)\n",
    "val_size = int(0.1 * DATASET_SIZE)\n",
    "\n",
    "\n",
    "train_ds = dict_ds.take(train_size)\n",
    "val_ds = dict_ds.skip(train_size).take(val_size)\n",
    "test_ds = dict_ds.skip(train_size).skip(val_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.take(5):\n",
    "    inputs, outputs = batch\n",
    "    # outputs = outputs[...,tf.newaxis]\n",
    "    print(len(inputs), len(outputs))\n",
    "\n",
    "print('Input: ', inputs[0].shape)\n",
    "print('Input: ', inputs[1].shape)\n",
    "print('Output: ', outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Confirmamos los tamaños de los nuevos conjuntos de datos particionados en lotes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Ahora no es necesario separar en atributos (`X`) y targets (`y`), solo debemos pasar los `Datasets` de entrenamiento o prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Adaptar para normalizar los atributos numéricos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numeric features\n",
    "normalize_layer = keras.layers.Normalization(axis=2)\n",
    "normalize_layer.adapt(train_ds.map(lambda X, y: X[0]))\n",
    "normalize_layer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Podemos conocer la media y la varianza que luego podríamos usar para invertir los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DEMANDA TOTAL - Mean: {0:.0f}, Std.Dev {1:.0f}'.format(normalize_layer.mean[0,0,0],\n",
    "                                                           np.sqrt(normalize_layer.variance[0,0,0])))\n",
    "print('Temperatura - Mean: {0:.1f}, Std.Dev. {1:.1f}'.format(normalize_layer.mean[0,0,1],\n",
    "                                                             np.sqrt(normalize_layer.variance[0,0,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Agregando la capa custom de preprocesado para los atributos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our definition for the categorical preporcossing layer\n",
    "class OneHotPre(keras.layers.Layer):\n",
    "    def __init__(self, max_tokens, vocabulary=None, num_oov_indices=0, **kwargs):\n",
    "        self._lower_lyr = tf.keras.layers.Lambda(lambda x: tf.strings.lower(x))\n",
    "        self._lab_lyr = tf.keras.layers.Lambda(lambda x: \n",
    "                                           tf.strings.regex_replace(x, '^.*h.bil.*$', 'laborable'))\n",
    "        self._semilab_lyr = tf.keras.layers.Lambda(lambda x:\n",
    "                                                   tf.strings.regex_replace(x,'^.*semilaborable.*$', 'semilaborable'))\n",
    "        self._nolab_lyr = tf.keras.layers.Lambda(lambda x:\n",
    "                                                 tf.strings.regex_replace(x,'^.*feriado.*$', 'nolaborable'))\n",
    "        self._lookup_layer = tf.keras.layers.StringLookup(\n",
    "            max_tokens = max_tokens,\n",
    "            num_oov_indices = num_oov_indices,\n",
    "            vocabulary = vocabulary,\n",
    "            # output_mode = 'int')\n",
    "            output_mode = 'one_hot')\n",
    "        super().__init__(**kwargs)\n",
    "    def _to_day_type(self, value):\n",
    "        day_type = self._lower_lyr(value)\n",
    "        day_type = self._lab_lyr(day_type)\n",
    "        day_type = self._semilab_lyr(day_type)\n",
    "        return self._nolab_lyr(day_type)\n",
    "    def adapt(self, data_sample):\n",
    "        map_ds = data_sample.map(lambda x: self._lower_lyr(x))\n",
    "        map_ds = map_ds.map(lambda x: self._lab_lyr(x))\n",
    "        map_ds = map_ds.map(lambda x: self._semilab_lyr(x))\n",
    "        map_ds = map_ds.map(lambda x: self._nolab_lyr(x))\n",
    "        self._lookup_layer.adapt(map_ds)\n",
    "        # self._lookup_layer.adapt(data_sample)\n",
    "    def call(self, inputs):\n",
    "        # return self._lookup_layer(self._to_day_type(inputs))\n",
    "        ll = self._lower_lyr(inputs)\n",
    "        ll = self._lab_lyr(ll)\n",
    "        ll = self._semilab_lyr(ll)\n",
    "        ll = self._nolab_lyr(ll)\n",
    "        return self._lookup_layer(ll)\n",
    "    def get_config(self):\n",
    "        return {'vocabulary': self._lookup_layer.get_vocabulary()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Adaptando la capa custom para preprocesar los attributos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "one_hot_layer = OneHotPre(3)\n",
    "one_hot_layer.adapt(train_ds.map(lambda X, y: X[1]))\n",
    "one_hot_layer.get_config()\n",
    "\n",
    "#one_hot_layer = OneHotPre(index, 7, ['Domingo o Feriado',\n",
    "#                                     'Sabado o Semilaborable', \n",
    "#                                     'Miercoles habiles', \n",
    "#                                     'Jueves Habiles', \n",
    "#                                     'Martes Habiles',\n",
    "#                                     'Viernes habiles',\n",
    "#                                     'Lunes habiles'])\n",
    "one_hot_layer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    inputs, outputs = batch\n",
    "    print(inputs[1].shape)\n",
    "    print(inputs[0].shape)\n",
    "    other=one_hot_layer(inputs[1])\n",
    "    other2=normalize_layer(inputs[0])\n",
    "    print(other.shape)\n",
    "    print(other2.shape)\n",
    "    print(other.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Definición del modelo \n",
    "\n",
    "\n",
    "\n",
    "En este caso, definimos el modelo de manera que procese la información de entrada por 2 canales diferentes. Los datos numéricos se normalizan, los categóricos pasan por la capa custom de codificación one-hot. Luego, ambas salidas se unen para continuar su procesamiento en la primer capa con celdas LSTM. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Como pasar multiples datos de entrada desde un Dataset\n",
    "\n",
    "TF: Training & evaluation with the built-in methods \n",
    "\n",
    "the Dataset should return a tuple of dicts.\n",
    "\n",
    "[Passing data to multi-input, multi-output models](https://www.tensorflow.org/guide/keras/training_with_built_in_methods#passing_data_to_multi-input_multi-output_models)\n",
    "\n",
    "y también ver TF: The Functional API\n",
    "\n",
    "When calling fit with a Dataset object, it should yield either a tuple of lists like `([title_data, body_data, tags_data], [priority_targets, dept_targets])` or a tuple of dictionaries like `({'title': title_data, 'body': body_data, 'tags': tags_data}, {'priority': priority_targets, 'department': dept_targets})`.\n",
    "\n",
    "[Manipulate complex graph topologies](https://www.tensorflow.org/guide/keras/functional_api#manipulate_complex_graph_topologies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalize_layer = keras.layers.Normalization(axis=1, mean=normalize_layer.mean[0,0,0],\n",
    "                                              variance=normalize_layer.variance[0,0,0],\n",
    "                                               invert=True, name='output')\n",
    "\n",
    "# Functional API Model \n",
    "numeric_input = keras.layers.Input(shape=(SEQ_LENGTH, 2), dtype=tf.float32, name='num_input')\n",
    "normalized = normalize_layer(numeric_input)\n",
    "categorical_input = keras.layers.Input(shape=(SEQ_LENGTH, 1), dtype=tf.string, name='cat_input')\n",
    "encoded = one_hot_layer(categorical_input)\n",
    "reshape_layer = tf.keras.layers.Reshape((SEQ_LENGTH,3))(encoded)\n",
    "concat = keras.layers.concatenate([normalized, reshape_layer])\n",
    "lstm_lyr = keras.layers.LSTM(10)(concat)\n",
    "dropout_lyr = keras.layers.Dropout(0.2)(lstm_lyr)\n",
    "dense_lyr = keras.layers.Dense(1, activation='linear')(dropout_lyr)\n",
    "unnormalized = unnormalize_layer(dense_lyr)\n",
    "model_exo = keras.models.Model(inputs=[numeric_input, categorical_input], outputs=[unnormalized])\n",
    "model_exo.compile(loss = 'MeanSquaredError', metrics=['MAE'], optimizer='Adam')\n",
    "# model_exo.compile(loss = {'output': tf.keras.losses.MeanSquaredError()},\n",
    "#                  metrics={'output': tf.keras.metrics.MeanAbsoluteError()},\n",
    "#                  optimizer='Adam')\n",
    "model_exo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "El método `.summary()` nos muestra cómo fluirá la información entre las capas de NN y cuántos parámetros se deberán ajustar en cada una. Otra alternativa para representar la configuración de la NN es usar `tf.keras.utils.plot_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model_exo, rankdir=\"TB\", show_shapes=True,  show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Ajuste del modelo\n",
    "\n",
    "Entrenamos el modelo, es decir, ajustamos los pesos de acuerdo al esquema definido cuando fue compilado, por ejemplo optimizer='Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "history = model_exo.fit(train_ds, epochs=30, validation_data=val_ds)\n",
    "\n",
    "# early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "# history = model_exo.fit(train_ds, epochs=3000, validation_data=val_ds, callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Para tener una idea sobre la convergencia del moelo, mostramos las curvas de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "# plt.gca().set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Luego que el modelo ha sido ajustado, es muy fácil obtener nuevas predicciones, como por ejemplo cuando le pasamos el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict discards targets\n",
    "predicted_values = model_exo.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_values = test_ds.map(lambda X, y: y['output'])\n",
    "#target_values = target_values.flat_map(tf.data.Dataset.from_tensor_slices)\n",
    "#print(type(target_values))\n",
    "#target_values = np.fromiter(target_values.as_numpy_iterator(), dtype=np.float32)\n",
    "#print(target_values.shape)\n",
    "\n",
    "# or\n",
    "target_values = test_ds.map(lambda X, y: y)\n",
    "target_values = target_values.unbatch()\n",
    "print(type(target_values))\n",
    "target_values = list(target_values.as_numpy_iterator())\n",
    "print(len(target_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Evaluación de los resultados\n",
    "\n",
    "Ahora vamos a representar nuestro resultado. Construimos un `DataFrame` para almacenar los valores de `DEMANDA TOTAL` predecida y observada, y su fecha (que recuperamos de los datos originales). Este `DataFrame` nos facilitará el cálculo de distintas métricas que califican el grado de acuerdo de las predicciones del modelo con los valores registrados. Asimismo, representaremos en una figura ambas series de datos para tener una estimación visual del desempeño del modelo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast Plot with Dates on X-axis\n",
    "LSTM_eval = pd.DataFrame({\n",
    "    'Predicted_DEMANDA': predicted_values[:, 0],\n",
    "    'Actual_DEMANDA': target_values,\n",
    "})\n",
    "\n",
    "LSTM_eval.loc[:, 'Date'] = dataFrame['Fecha'][-len(target_values):].values\n",
    "# LSTM_eval.set_index('Date', inplace=True)\n",
    "LSTM_eval['Date'] = pd.to_datetime(LSTM_eval['Date'])\n",
    "LSTM_eval.set_index('Date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una figura para representar las series de datos\n",
    "def FigPredActual(d, title):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    #  highlight the  forecast\n",
    "    # highlight_start = int(len(d) * 0.9)  \n",
    "    # highlight_end = len(d) - 1  # Adjusted to stay within bounds\n",
    "    # Plot the actual values\n",
    "    # plt.plot(d[['Actual_DEMANDA']][:highlight_start], label=['Actual_DEMANDA'])\n",
    "    plt.plot(d[['Actual_DEMANDA']], label=['Actual_DEMANDA'])\n",
    "    \n",
    "    # Plot predicted values with a dashed line\n",
    "    plt.plot(d[['Predicted_DEMANDA']], label=['Predicted_DEMANDA'], linestyle='--')\n",
    "    \n",
    "    # Highlight the forecasted portion with a different color\n",
    "    # plt.axvspan(d.index[highlight_start], d.index[highlight_end], facecolor='lightgreen', alpha=0.5, label='Forecast')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Dates')\n",
    "    plt.ylabel('Values')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Trazamos ambas series de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigPredActual(LSTM_eval, 'Multivariate Time-Series forecasting using LSTM and Exogenous variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Definimos una función conveniente para calcular diferentes métrias que nos informan sobre el grado de acuerdo de las predicciones del modelo con las observaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def performance(d):\n",
    "    return {\n",
    "        'MSE': mean_squared_error(d['Actual_DEMANDA'].to_numpy(), d['Predicted_DEMANDA'].to_numpy()),\n",
    "        'MAE': mean_absolute_error(d['Actual_DEMANDA'].to_numpy(), d['Predicted_DEMANDA'].to_numpy()),\n",
    "        'R2': r2_score(d['Actual_DEMANDA'].to_numpy(), d['Predicted_DEMANDA'].to_numpy())\n",
    "    }\n",
    "\n",
    "performance(LSTM_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "**Preguntas**\n",
    "\n",
    "1. ¿Por qué el ajuste mejora si `BATCH_SIZE` y `SEQ_LENGTH` coinciden (sin importar demasiado si el número de días corresponde a un número entero de semanas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.12-TF2.18",
   "language": "python",
   "name": "venv-tf2.18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
