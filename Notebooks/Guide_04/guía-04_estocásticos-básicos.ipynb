{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Guía 4 - Modelos estocásticos básicos\n",
    "\n",
    "## Algunas consideraciones\n",
    "\n",
    "* Si el modelo encierra la mayoría de los atributos determinísticos de la serie temporal (TS), entonces la serie de los residuos debería verse como una realización de una variable aleatoria independiente.\n",
    "* Si permanece cierta estructura residual, entonces esta información podría aprovecharse para mejorar los modelos.\n",
    "* Si esperamos que la serie de los residuos sea una realización de una variable aleatoria independiente, entonces parece natural construir modelos sobre variaciones aleatorias independientes (*white noise o ruido blanco*).\n",
    "\n",
    "*Asumimos que la serie de los residuos es estacionaria (si no, los coeficientes de los modelos cambiarían).*\n",
    "\n",
    "## Ruido blanco\n",
    "\n",
    "Una TS $\\{w_t\\}$ es un ruido blanco si las variables $w_1, \\ldots, w_N$ son independientes e idénticamente distribuídas (iid) con media nula ($\\mu=0$) y la autocovarianza cumple\n",
    "\n",
    "$$\\mathrm{Cov}(w_t, w_{t+k}) = \\left\\{ \n",
    "\\begin{array}{ll}\n",
    "\\sigma^2 & k=0\\,  \\mathrm{todo}\\,  t \\\\\n",
    "0        & k \\ne 0 \n",
    "\\end{array}\n",
    "\\right.$$\n",
    "\n",
    "Además, si las variables siguen una distribución normal se dice que es un ruido blanco gaussiano. Más aú, las propiedades estacionarias de segundo orden son una consecuencia inmediata de la definición (ni la media $\\mu$, la varianza $\\mathrm{Var}(w_t)=\\mathrm{Cov}(w_t, w_t) = \\mathrm{Cov}(0)=\\sigma^2$ o las distntas autocovarianzas $\\mathrm{Cov}_w(k)=0$ dependen de $t$). \n",
    "\n",
    "\n",
    "## Ejercicios\n",
    "\n",
    "1. Generar una serie sintética de ruido blanco. Notar que la biblioteca `numpy` aconseja instanciar `random.Generator.normal` en lugar del antiguo comando `random.normal`. Los generadores de números pseudo-aleatorios (PRNG), típicamente, con una distribución uniforme entre 0 y 1 son la base para obtener, mediante diferentes transformaciones, series que siguen diferentes distribuciones. Las buenas propiedades de los PNRG son centrales para simular TS (ver, por ejemplo, el capítulo 7 de [Numerical Recipes](https://numerical.recipes/book.html))  \n",
    "    a. Representar la función de acf.  \n",
    "    b. Observar que un 5% de las autocorrelaciones resultan significativamente distintas de cero. Chequear que esta observación es compatible con el 5% de significancia estadística. ¿Cómo cambiaría si se elige un umbral de 1% de significancia estadística?  \n",
    "2. Examinar la serie de registros meteorológicos de la estación castelar. Mostar que las series anualizadas de `precip`, `tempmax` y `tempmin` pueden ser compatibles con un ruido blanco.  \n",
    "    a. Hallar la tendencia con una regresión lineal y mostrar que la temperatura máxima (media anual) aumentaría cerca de una décima de grado por década. ¿Qué confianza se puede asignar a esta estimación? ¿Se puede rechazar la hipótesis nula (que la temperatura no aumenta con el tiempo)?  \n",
    "    b. Probar que las series anualizadas son estacionarias.  \n",
    "    c. Representar las autocorrelaciones.  \n",
    "    d. ¿Se pueden realizar otros tests que muestren que las TS (removiendo la media) son compatibles con realizaciones de una variable aleatoria e independiente?  \n",
    "  \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Ayudas para el ejercicio 2\n",
    "\n",
    "Comenzamos importando, como es habitual, las bibliotecas de `numpy`, `pandas` y de gráficos `matplotlib`. Además, definimos una función que ayuda en la lectura e intepretación de la prueba de DIckey-Fuller aumentada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(timeseries):\n",
    "    print(\"Results of Dickey-Fuller Test:\")\n",
    "    dftest = adfuller(timeseries, autolag=\"AIC\")\n",
    "    dfoutput = pd.Series(\n",
    "        dftest[0:4],\n",
    "        index=[\n",
    "            \"Test Statistic\",\n",
    "            \"p-value\",\n",
    "            \"#Lags Used\",\n",
    "            \"Number of Observations Used\",\n",
    "        ],\n",
    "    )\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput[\"Critical Value (%s)\" % key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Cargamos los datos de la estación castelar que almacena los registros a paso diario desde 1970 para la precipitación acumulada en 24 horas, la temperatura máxima y la temperatura mínima diaria. Luego, empleamos el método `resample()` para obtener las medias anuales en los casos de temperatura, y el acumulado anual de lluvias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "castelar = pd.read_csv('Data/castelar_1970_2011.csv')\n",
    "castelar.columns = castelar.columns.str.strip()\n",
    "date_string = castelar['Year'].astype(str) + '-' + castelar['Month'].astype(str) + '-' + castelar['Day'].astype(str)\n",
    "castelar.set_index(pd.to_datetime(date_string, format='%Y-%m-%d'), inplace=True)\n",
    "castelar.asfreq('D')\n",
    "castelar.drop(columns=['Year','Month','Day'], inplace=True)\n",
    "castelar.replace(-1000, np.nan, inplace=True)\n",
    "print(f'Number of rows with missing values: {castelar.isnull().any(axis=1).sum()}')\n",
    "castelar_anual = castelar.resample('1YE').agg({'precip':'sum', 'tempmax':'mean', 'tempmin':'mean'}).asfreq('YE')\n",
    "castelar_anual.plot(subplots=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Finalmente, dejamos el cálculo para obtener la tendencia y el análisis para la hipótesis nula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# Extraemos los días que pasaron desde el inicio de la serie para cada lectura. Luego, removemos la tendencia\n",
    "interval = np.float32((castelar_anual.index - castelar_anual.index[0]).days.values)/365.2425\n",
    "res = stats.linregress(interval, castelar_anual['tempmax'])\n",
    "print(f'Intercept: {res.intercept:.2f}°C')\n",
    "\n",
    "# Two-sided inverse Students t-distribution\n",
    "# p - probability, df - degrees of freedom\n",
    "import scipy.stats as sstats\n",
    "tinv = lambda p, df: abs(sstats.t.ppf(p/2, df))\n",
    "ts = tinv(0.05, len(interval)-2)\n",
    "print(f\"slope (95%): ({res.slope:.3f} +/- {ts*res.stderr:.3f})°C/año\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Notación: Operador de retroceso en el tiempo\n",
    "\n",
    "El *backward shift* u operador de traslación en reversa en el tiempo se define como\n",
    "\n",
    "$$\\mathbf{B} x_t = X_{t-1}$$\n",
    "\n",
    "y su aplicación reiterada $n$ veces es\n",
    "\n",
    "$$\\mathbf{B}^n x_t = X_{t-n}$$\n",
    "\n",
    "## Modelos autoregresivos\n",
    "\n",
    "La serie $\\{x_t\\}$ es un proceso autoregresivo de orden AR(p) si \n",
    "\n",
    "$$x_t = \\alpha_1 x_{t-1} + \\cdots + \\alpha_p x_{t-p} + w_t$$\n",
    "\n",
    "donde  $\\{w_t\\}$ es un ruido blanco, $\\alpha_i$ son los parámetros del modelo y $\\alpha_p$ debe ser distinto de cero.\n",
    "\n",
    "Aplicando el opoerador de traslación temporal el modelo AR(p) puede expresarse de manera compacta como un polinomio $\\Theta$ en el operador $\\matbf{B}$\n",
    "\n",
    "$$\\Theta (\\mathbf{B}) x_t = 1 - \\alpha_1 x_{t-1} - \\cdots - \\alpha_p x_{t-p} = w_t$$\n",
    "\n",
    "\n",
    "### Características\n",
    "\n",
    "\n",
    "a. El *random walk o camino aleatorio* es un caso especial de AR(1) con $\\alpha_1=1$ de manera que resulta\n",
    "\n",
    "$$x_t = x_{t-1} + w_t$$\n",
    "\n",
    "b. El *exponential moving average o suavizado exponencial* es un caso especial con $\\alpha_i = \\alpha(1-\\alpha)^i$ para $i=1,2,\\ldots,p\\rightarrow\\infty$\n",
    "\n",
    "$$u_t = \\alpha x_t + (1-\\alpha) u_{t-1}$$\n",
    "\n",
    "donde $u_t$ es la media suavizada que resulta de las $p$ observaciones $x_t$ anteriores. Es fácil mostrar por sustitución hacia atrás que\n",
    "\n",
    "$$u_t = \\sum_{i=0}^{p\\rightarrow\\infty} \\alpha (1-\\alpha)^i x_{t-i}$$\n",
    "\n",
    "c. El modelo es una regresión de $x_t$ en sus valores anteriores, de ahí su nombre. \n",
    "\n",
    "d. Una predicción o pronóstico para el tiempo $t$ dados los $p$ valores hasta $t-1$ se expresa como\n",
    "\n",
    "$$\\hat{x}_{t|t-1} = \\alpha_1 x_{t-1} + \\cdots + \\alpha_p x_{t-p}$$\n",
    "\n",
    "e. Los parámetros $\\alpha$ pueden obtenerese por minimización de la suma al cuadrado de los residuos (RSS).\n",
    "\n",
    "f. Las raíces del polinomio carcterístico $\\Theta (\\mathbf{B}) =0$ tomando a $\\mathbf{B}$ como un escalar deben ser mayores a 1 en valor absoluto para que el proceo sea estacionario.  \n",
    "    * El random walk no es estacionario (de hecho se puede demostrar que la varianza cambia como $\\mathrm{Var}(x)=t\\sigma^2$)   \n",
    "\n",
    "$$\\Theta (\\mathbf{B}) = 1 - \\mathbf{B} = 0  \\Rightarrow \\mathbf{B} = 1$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "3. Simular un proceso AR(3) con $\\alpha_1= 0.95, \\alpha_2=-0.275, \\alpha_3=0.025$.  \n",
    "    a. Obtener la serie sintética para un proceso AR(3).  \n",
    "    b. Probar que la serie resultante es estacionaria. ¿Cuáles son las raíces del polinomio carcterístico?  \n",
    "    c. Obtener el gráfico para la función de autocorrelación y para la función de autocorrelación parcial.  \n",
    "    d. Encontrar el modelo AR(p) que mejor ajusta a la serie sintética empleando el criterio de de información de Akaike (AIC)  \n",
    "\n",
    "4. Ajustar un modelo AR(p) a la serie de niveles hidrométricos promediados anualmente para la estación hidrológica sobre el río Paraná en Corrientes.  \n",
    "    a. Separar en datos de entrenamiento y prueba.  \n",
    "    b. Probar que la serie resultante es estacionaria.  \n",
    "    c. Obtener el gráfico para la función de autocorrelación y para la función de autocorrelación parcial.  \n",
    "    d. Encontrar el modelo AR(p) que mejor ajusta a la serie sintética empleando el criterio de de información de Akaike (AIC).\n",
    "    e. Evaluar el desempeño del modelo con una métrica como el RMSE. Compara con un modelo base que repite el valor del año anterior.  \n",
    "\n",
    "6. Simular un proceso AR(1).  \n",
    "    a. Obtener el gráfico para la función de autocorrelación. Mostrar que sigue una dependencia $\\mathrm{Cor}(k) \\sim \\alpha^k$.  \n",
    "    b. Obtener el gráfico para la función de autocorrelación parcial y observar que no tiene valores significativos salvo para el lag 1.  \n",
    "\n",
    "7. Comparar como varían la media y la varianza para un proceso estacionario y otro no estacionario.  \n",
    "    a. Tomar como proceso estacionario un modelo AR(1) con $\\alpha=0.5$, es decir $x_t=0.5 x_{t-1} + w_t$.  \n",
    "    b. Tomar como proceso no estacionario un random walk, es decir $x_t = x_{t-1} + w_t$.  \n",
    "    c. Calcular la media de cada TS en función de $t$. Es decir, calcular el promedio considerando los primeros $t$ valores de la serie. Repetir el paso anterior incluyendo ahora hasta el valor $t+1$. Continuar hasta emplear todos los valores de una TS con 1000 datos.   Realizar el gráfico de los promedios parciales en función de $t$. (hint: `np.cumsum(x)/(np.arange(x.size) + 1)`).  \n",
    "    d. De manera análoga al punto c, realizar el gráfico de los promedios parciales en función de $t$ para la varianza de ambos modelos.  \n",
    "    e. Interpretar la relación entre las raíces de los polinomios característicos de ambos modelos y su comportamiento (estacionario o no estacionario). Chequear los resultados con la prueba de Dickey-Fuller aumentada.  \n",
    "    f. Calcular la diferncia a un paso para el random walk `np.diff(x, 1)` y confirmar que la nueva serie es un ruido gaussiano como se espera de la definición $x_t - x_{t-1} = w_t$.\n",
    "\n",
    "  \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Ayuda para el ejercicio 3\n",
    "\n",
    "Comenzaremos generando una serie sintética para un proceso AR(4) con ayuda de la biblioteca `statsmodels.tsa.arima_process`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.arima_process import arma_generate_sample\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "arparams = np.array([0.95, -0.275, 0.025])\n",
    "maparams = np.array([])\n",
    "ar_params = np.r_[1, -arparams] # add zero-lag and negate\n",
    "ma_params = np.r_[1, maparams] # add zero-lag\n",
    "\n",
    "n_samples = 250\n",
    "\n",
    "# Se puede genera una TS con\n",
    "#y = arma_generate_sample(\n",
    "#    ar=ar_params, \n",
    "#    ma=ma_params, \n",
    "#    nsample=n_samples, \n",
    "#    scale=1            # Standard deviation of the noise\n",
    "#)\n",
    "# o también\n",
    "arma_process = ArmaProcess(ar_params, ma_params)\n",
    "arma_sample = arma_process.generate_sample(n_samples, scale=1.0)\n",
    "\n",
    "\n",
    "y = pd.Series(arma_sample, index=pd.date_range(start='2026-01-01 00:00:00', periods=n_samples, freq='1h'))\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(y)\n",
    "# Use DateFormatter to show only the hour (%H)\n",
    "hour_form = mdates.DateFormatter(\"%H:%M\")\n",
    "ax.xaxis.set_major_formatter(hour_form)\n",
    "\n",
    "# Optional: Adjust locator to place ticks at the start of each year (or at intervals)\n",
    "ax.xaxis.set_major_locator(mdates.HourLocator(interval=12))\n",
    "\n",
    "# Optional: Improve label presentation (e.g., rotation)\n",
    "fig.autofmt_xdate(rotation=45, ha='right')\n",
    "ax.set_title(\"Simulated ARIMA(3, 0, 0) Time Series\")\n",
    "ax.set_xlabel(\"Time Step\")\n",
    "ax.set_ylabel(\"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Realizamos la prueba de Dickey-Fuller aumentado para confirmar que la serie es estacionaria. La expresión para la serie con los coeficientes elegidos es\n",
    "\n",
    "$$x_t = 0.95 x_{t-1} - 0.275 x_{t-2} + 0.025 x_{t-3} + w_t$$\n",
    "\n",
    "cuyas raíces son 2, 4 y 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Representamos las funciones de autocorrelación parcial y autocorrelación parcial. por construcción, esperamos que la función de autocorrelación parcial no tenga valores distintos de cero con significancia estadística despué del lag 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(y, lags=40, bartlett_confint=False)\n",
    "plt.xlabel('Lags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(y, lags=40)\n",
    "plt.xlabel('Lags')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Ensayamos el ajuste al modelo esperado con la clase `ARIMA`. Notamos que a diferencia de ` sklearn`, luego de ajustar el modelo con el método `fit()`, obtenemos otra clase denominada `ARIMAResult` sobre la que extraeremos las predicciones y otras propiedades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(y, order=(3, 0, 0), trend='n')\n",
    "res = model.fit()\n",
    "print('Sumary: ', res.summary())\n",
    "print('Params: ', model.params_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Por ejemplo, luego que se ha ajustado el modelo y obtenido sus parámetros se puede generar una TS sintética en base a esa configuración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data starting from the end of the original time series\n",
    "# Simulate 100 new observations\n",
    "n_simulations = 100\n",
    "simulated_series = res.simulate(n_simulations, anchor='end')\n",
    "\n",
    "# Plot original and simulated data\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(y,label='Original Data', color='k')\n",
    "ax.plot(simulated_series, label='Simulated Data', color='C0', alpha=0.5)\n",
    "ax.set_title(\"Original and Simulated Time Series from fitted ARIMA(2,0,2)\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "El método `predict()` nos da las prediciones en los pasos de tiempo cargados incialmenente en la definición del modelo `ARIMA` (no hace falta pasar la variable endógena como pensaríamos de `sklearn`. Asimismo, se puede pronosticar con el método `forecast(n)`  n-pasos delante. Ciertamente, al no coniderar el ruido, el pronóstico converge a la media del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values with fitted model\n",
    "predicted_series = res.get_prediction().predicted_mean\n",
    "\n",
    "# forecast data starting from the end of the original time series\n",
    "# Forecast 100 new observations\n",
    "n_forecasts = 100\n",
    "forecasted_series = res.get_forecast(n_forecasts).predicted_mean\n",
    "\n",
    "# Plot original and simulated data\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(y,label='Original Data', color='k')\n",
    "ax.plot(forecasted_series, label='Forecasted Data', color='C0', alpha=0.5)\n",
    "ax.plot(predicted_series, label='Predicted Data', color='C1', alpha=0.5)\n",
    "ax.set_title(\"Original, Predicted and Forecasted Time Series from fitted ARIMA(2,0,2)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Si no conocemos el orden del modelo AR(p), para hallar el $p$ que logra el mejor ajuste utilizaremos el criterio de información de Akaike que busca maximizar el logaritmo de la verosimilitud (*likelihood*) pero penaliza la abundancia de parámetros \n",
    "\n",
    "AIC = -2 log-likelihood + 2 cantidad-de-parámetros\n",
    "\n",
    "La verosimilitud medirá la probabilidad de observar un conjunto de datos dado un modelo con sus parámetros ajustados. Es decir, que la verosimilitud invierte la lógica. Dado un conjunto de datos observado, la verosimilitud estimará que tan probable es que diferentes parámetros del modelo generen la sequencia observada. Esta función puede pensarse como una respuesta a la pregunta de *\"¿qué tan probable es que mis datos observados provengan de un dado modelo ARMA(p,q) propuesto?\"*.\n",
    "\n",
    "AIC mantiene un balance entre subajuste (*underfitting*) y sobreajuste (*overfitting*). Así, es una medida relativa entre diferentes modelos. no dice cuál es el modelo bueno sino cuál es el más adeucado entre los propuestos. \n",
    "\n",
    "A partir de estos argumentos, planteamos una iteración para valores crecientes de $p$ y nos quedaremos con aquel modelo que obtenga el menor AIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_max = 6\n",
    "\n",
    "aic_array = np.empty(p_max)\n",
    "mse_array = np.empty(p_max)\n",
    "\n",
    "for p in range(p_max):\n",
    "    model = ARIMA(y, order=(p, 0, 0), trend='n')\n",
    "    res = model.fit(method='yule_walker')\n",
    "    aic_array[p] = res.aic\n",
    "    mse_array[p] = res.mse\n",
    "    if p == 0:\n",
    "        aic_min = res.aic\n",
    "        p_best = 0\n",
    "    else:\n",
    "        if aic_min > res.aic:\n",
    "            aic_min = res.aic\n",
    "            p_best = p\n",
    "\n",
    "print(f'Best order: ({p_best}, 0, 0)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Vemos que, si bien no se ha reobtenido el orden propuesto inicialmente, el criterio de Akaike nos deja un modelo similar. Buscar el mínimo del MSE no es suficiente ya que, al crecer el orden, siempre disminuyen los residuos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mse_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Ayuda para el ejercicio 4\n",
    "\n",
    "Analizar la serie histórica de niveles (alturas en metros) registrados en la estación hidrométrica Corrientes de la Red Hidrológica Nacional consultada desde [Sistema Nacional de Información Hídrica](https://snih.hidricosargentina.gob.ar/Filtros.aspx). En particular, nos quedaremos con los promedios anuales desde el año 1910. La estimación y pronóstico de alturas hidrométricas tiene mucho impacto ya que los niveles bajos limitan la navegación del Paraná y la toma de aguas, mientras que los altos afectan a las poblaciones ribereñas.\n",
    "\n",
    "Comenzamos cargando la base de datos de alturas. Notamos que algunas fechas cuentan con varias lecturas por lo que forzamos a un valor medio diario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrientes = pd.read_excel('Data/niveles_Corrientes.xlsx', skiprows=1)\n",
    "print('Nombre de las columnas:', corrientes.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construímos la serie de alturas diarias\n",
    "# coerce convierte a np.nan datos que no pueden ser transformados a float\n",
    "# resample nos asegura que si hay más de un dato diario, se quedará con el promedio\n",
    "# asfreq nos completa las fechas, eventualmente con np.nan si no hay dato\n",
    "dti = pd.to_datetime(corrientes['Fecha y Hora'], format='%d/%m/%Y %H:%M')\n",
    "niveles = pd.Series(pd.to_numeric(corrientes['Altura [m]'], errors='coerce', downcast='float').values, index=dti)\n",
    "niveles_diarios = niveles.resample('1d', origin='1910-01-01').mean().asfreq('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(niveles_diarios[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeamos si hay muchos años con un faltante importante de datos\n",
    "suma_nans = niveles_diarios.isna().resample('YE').sum().asfreq('YE')\n",
    "suma_nans[suma_nans > 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Finalmente, nos interesa recuperar los promedios de altura anuales para estudiar si corresponden a un proceso estacionario autoregresivo y, eventualmente, ajustar un modelo AR(p)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con los promedios anuales\n",
    "niveles_anuales = niveles_diarios.resample('YE').mean().asfreq('YE')\n",
    "niveles_anuales.index.rename('Year', inplace=True)\n",
    "niveles_anuales.rename('Altura [m]', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Separamos en datos de entrenamiento y prueba. Realizamos un gráfico para ayudarnos a interpretar el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_train = niveles_anuales[:int(0.8*niveles_anuales.size)]\n",
    "na_test = niveles_anuales[int(0.8*niveles_anuales.size):]\n",
    "\n",
    "na_train.plot(label='Train', c='C0')\n",
    "na_test.plot(label='Test', c='C1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Ahora representamos las funciones de autocorrelación y autocorrelación parcial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(na_train, lags=30, bartlett_confint=False)\n",
    "plt.xlabel('Lags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(na_train, lags=30)\n",
    "plt.xlabel('Lags')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Observamos que la función de autocorrelación parcial registra un valor alto con significacnia estadística solo para el primer lag, sugiriendo un modelo AR(1).\n",
    "A su vez, chequeamos que la serie de alturas sea estacionaria para poder avanzar con el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(na_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "La serie es estacionaria... probamos si se puede ajustar con un modelo AR(p)\n",
    "\n",
    "Vamos a iterar sobre distintos $p$ de manera de quedarnos con aquel modelo que logra el AIC más bajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_max = 10\n",
    "aic_array = np.empty(p_max)\n",
    "\n",
    "for p in range(p_max):\n",
    "    model = ARIMA(na_train, order=(p, 0, 0), trend='c')\n",
    "    res = model.fit(method='yule_walker')\n",
    "    aic_array[p] = res.aic\n",
    "    if p == 0:\n",
    "        aic_min = res.aic\n",
    "        p_best = 0\n",
    "    else:\n",
    "        if aic_min > res.aic:\n",
    "            aic_min = res.aic\n",
    "            p_best = p\n",
    "\n",
    "print(f'Best order: ({p_best}, 0, 0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Representamos la serie de alturas (promedio anual) de la estación hidrológica Corrientes y las comparamos con las predicciones del modelo AR(1) tanto con datos de entrenamiento como de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(na_train, order=(p_best, 0, 0), trend='c')\n",
    "res_train = model.fit(method='yule_walker')\n",
    "# Predicted values with fitted model\n",
    "train_predicted_series = res_train.get_prediction().predicted_mean[p_best:]\n",
    "\n",
    "res_test = res_train.extend(na_test)\n",
    "test_predicted_series = res_test.fittedvalues\n",
    "\n",
    "# Plot original and simulated data\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(na_train,label='Original Train Data', color='C0')\n",
    "ax.plot(na_test,label='Original test Data', color='C1')\n",
    "ax.plot(train_predicted_series, label='Predicted Train Data', color='C0', alpha=0.5, linestyle='-.')\n",
    "ax.plot(test_predicted_series, label='Predicted Test Data', color='C1', alpha=0.5, linestyle='-.')\n",
    "ax.axvspan(na_test.index[0],na_test.index[-1],color='#808080', alpha=0.2)\n",
    "ax.set_title(f'Original and predicted Time Series from fitted ARIMA({p_best},0,0)')\n",
    "ax.set_xlabel('Year')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Finalmente, evaluamos el desempeño del modelo en entrenamiento y prueba, y lo comparamos con un modelo base que la predicción toma el valor registrado en el año inmediato anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ARMA Model Train Data RMSE: {np.sqrt(res_train.mse):.3f}')\n",
    "print(f'ARMA Model Test Data RMSE: {np.sqrt(res_test.mse):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ly_train = na_train.shift(1)\n",
    "ly_test = na_test.shift(1)\n",
    "mse_train = np.mean((ly_train - ly_train.mean())**2)\n",
    "mse_test = np.mean((ly_test - ly_test.mean())**2)\n",
    "\n",
    "print(f'Last Year Model Train Data RMSE: {np.sqrt(mse_train):.3f}')\n",
    "print(f'Last Year Test Data RMSE: {np.sqrt(mse_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Para mayor confianza en la elección del orden del modelo, podemos explorar los residuos esperando que sean una realización de una variable aletoria independiente e identicamente distribuídas. Podemos tomar el resiudo directamente `res_train.resid` o usar `plot_diagnostic()` con la cual el paquete `stastmodel` nos facilita el análisis cualitativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train.plot_diagnostics(figsize=(10,8))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
