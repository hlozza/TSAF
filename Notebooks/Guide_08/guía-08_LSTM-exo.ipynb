{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Guía 8 - Pronósticos de demanda diaria con LSTM usando Keras y TensorFlow\n",
    "\n",
    "## Pronósticos a partir de la serie temporal de la demanda diaria incorporando variables exógenas\n",
    "\n",
    "La propuesta es dar pronósticos de la demanda diaria de energía eléctrica a partir de sus valores registrados en días anteriores y de otros atributos que podrían tener influencia (variables exógenas). Obtendremos los pronósticos a partir del entrenamiento de redes neuronales (Neural Networks, NN) usando celdas del tipo Long-Short-Time-Memory (LSTM) y de otras configuraciones incorporando las variables exógenas.\n",
    "\n",
    "Continuaremos con nuestro ejemplo tomando la base de datos de demanda diaria de energía eléctrica publicados por CAMMESA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Inicio de la programación para el análisis y pronóstico de la serie temporal\n",
    "\n",
    "Comenzamos importando las bibliotecas necesarias y confirmamos la versión de TensorFlow (TF) disponible en la instalación. Esto nos puede ayudar a interpretar algunas diferencias en el comportamiento de las funciones de TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(99)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Carga del conjunto de datos para su análisis\n",
    "\n",
    "Cargamos los datos de demanda diaria. DEMANDA TOTAL es la serie que queremos predecir (target). Esperamos que sus valores históricos nos provean información para su pronóstico. Además, tomaremos otros atributos que consideramos relevantes como `TEMPERATURA REFERENCIA MEDIA GBA °C` o `Tipo día` que nos informa si la jornada fue laborable, semilaborable o no laborable. Las columnas que se refieren a los registros de demanda para otras regiones argentinas no las consideraremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "dataFrame = pd.read_excel('Data/Base Demanda Diaria 2017 2024.xlsx', sheet_name='Datos Región', skiprows=4)  \n",
    "dataFrame.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Separación de los conjuntos de datos de entrenamiento y prueba\n",
    "\n",
    "Como es habitual, separamos el conjunto de datos en entrenamiento y prueba. Como sabemos, se reserva una porción del final de la serie temporal como datos de prueba. En este ejercicio, desdoblamos en datos numéricos y categóricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len_rate = 0.9\n",
    "len_df = len(dataFrame)\n",
    "\n",
    "# TensorFlow tensors require that all elements have the same dtype.\n",
    "numeric_feature_names = ['DEMANDA TOTAL', 'TEMPERATURA REFERENCIA MEDIA GBA °C']\n",
    "column_indices = {name: i for i, name in enumerate(numeric_feature_names)}\n",
    "numeric_tensor = np.float32(dataFrame[numeric_feature_names].to_numpy())\n",
    "# cast to tf defaults... float32\n",
    "\n",
    "categoric_feature_names = ['Tipo día']\n",
    "categoric_tensor = dataFrame[categoric_feature_names].to_numpy()\n",
    "\n",
    "train_num = numeric_tensor[:int(train_len_rate * len_df)]\n",
    "train_cat = categoric_tensor[:int(train_len_rate * len_df)]\n",
    "\n",
    "test_num = numeric_tensor[int(train_len_rate * len_df):]\n",
    "test_cat = categoric_tensor[int(train_len_rate * len_df):]\n",
    "print(train_cat.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Ventanas de datos\n",
    "\n",
    "Necesitamos particionar la serie temporal en lotes operables por las sucesivas capas de NN. Es importante notar que dentro de cada lote se conserva el orden temporal..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single step dataset preparation\n",
    "# The input of LSTM layer has a shape of (num_timesteps, num_features)\n",
    "def singleStepSampler(df, window, target_idx=0):\n",
    "    xRes = []\n",
    "    yRes = []\n",
    "    for i in range(0, len(df) - window):\n",
    "        res = []\n",
    "        for j in range(0, window):\n",
    "            res.append(df[i+j,:])\n",
    "        xRes.append(res)\n",
    "        yRes.append(df[i + window,target_idx])\n",
    "    return tf.convert_to_tensor(xRes), tf.convert_to_tensor(np.array(yRes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Aplicamos las ventanas a los datos de prueba y entrenamiento. Dentro de cada ventana se arman lotes de datos de tamaño BATCH_SIZE. Así, `X` guardará la información de los días previos de la demanda y otras variables numéricas, mientras que `y` almacenará la predicción de demanda para el día siguiente. Además, `cat` representará la información categórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# un ciclo completo de 4 semanas\n",
    "# igual cantidad de instancias de lunes que de domingos (4)\n",
    "BATCH_SIZE = 28 \n",
    "\n",
    "(X_train, y_train) = singleStepSampler(train_num, BATCH_SIZE)\n",
    "cat_train = singleStepSampler(train_cat, BATCH_SIZE)[0]\n",
    "(X_test, y_test) = singleStepSampler(test_num, BATCH_SIZE)\n",
    "cat_test = singleStepSampler(test_cat, BATCH_SIZE)[0]\n",
    "\n",
    "y_train = y_train[...,tf.newaxis]\n",
    "y_test = y_test[...,tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Confirmamos los tamaños de los nuevos conjuntos de datos particionados en lotes de tamaño `BATCH_SIZE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(cat_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(cat_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Preprocesamiento con TensorFlow\n",
    "\n",
    "La propuesta es realizar el preprocesamiento con los métodos y herramientas provistos por TensorFlow. Esto tiene la ventaja de que puede añadirse dentro del flujo del modelo. Otro camino alternativo es utilizar `pipeline` y `ColumnTransformer` de SciKit-Learn. \n",
    "\n",
    "### Capas de preprocesamiento de TensorFlow\n",
    "\n",
    "En primer lugar, debemos tener en cuenta que los `tf.tensor` solo puden almacenar valores de un mismo tipo `dtype` (al igual que los `np.array`). Este es el motivo detrás de la separación de las columnas que ya practicamos en el `pd.DataFrame` entre datos numéricos y categóricos. En adelante, cuando los datos son de tipos heterogéneo, deberán preprocesarse de manera particular. Esto puede realizarse de dos maneras difrentes con las herramientas de TF.\n",
    "\n",
    "Por un lado, se puede adelantar todo el preprocesado conviertientdo los datos a `tf.data.Dataset` o usar `tf.Transform`: ver por ejemplo [Preprocessing data with TensorFlow Transform](https://www.tensorflow.org/tfx/tutorials/transform/census).\n",
    "\n",
    "Por otro lado, TF permite incluir el preprocesado dentro de las primeras capas del modelo: ver por ejemplo [Load a pandas DataFrame](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe). Nosotros abordaremos esta segunda estrategia.\n",
    "\n",
    "El artículo [Structured data classification from scratch](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/) ejemplifica el uso de `layers` de Keras para su preprocesado independiente e incluye atributos numéricos y categóricos. Nosotros seguiremos un tratamiento similar aprovechando `layers` ya definidos como `tf.keras.layers.Normalization` y `tf.keras.layers.StringLookup` explicadas en [Working with preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers). Además, deberemos crear nuestras propias capas de preprocesado inspirados por [Building a One Hot Encoding Layer with TensorFlow](https://towardsdatascience.com/building-a-one-hot-encoding-layer-with-tensorflow-f907d686bf39). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Utilizando la capa de normalización\n",
    "\n",
    "Vamos a dar un ejemplo sobre cómo opera la capa de `tf.keras.layers.Normalization` (disponible en versiones de TF recientes). Como su nombre lo sugiere, normaliza los datos numéricos. Debemos anticiparle los datos que procesará para que estime los estadísticos adecuados que usará en el escalado. Esto lo realiza el método `.adapt()`. Bśicamente, realiza una transformación afín calculando previamente la media y la desviación estandard en la dirección del eje (`axis`) definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numeric features\n",
    "normalize_layer = keras.layers.Normalization(axis=2)\n",
    "normalize_layer.adapt(X_train)\n",
    "normalize_layer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Aquí damos un ejemplo del escalado del primer registro del primer lote de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_layer(X_train[0,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Podemos conocer la media y la varianza que luego podríamos usar para invertir los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DEMANDA TOTAL - Mean: {0:.0f}, Std.Dev {1:.0f}'.format(normalize_layer.mean[0,0,0],\n",
    "                                                           np.sqrt(normalize_layer.variance[0,0,0])))\n",
    "print('Temperatura - Mean: {0:.1f}, Std.Dev. {1:.1f}'.format(normalize_layer.mean[0,0,1],\n",
    "                                                             np.sqrt(normalize_layer.variance[0,0,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Agregamos un ejemplo de inversión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalize_layer = keras.layers.Normalization(axis=2, mean=normalize_layer.mean,\n",
    "                                              variance=normalize_layer.variance, invert=True)\n",
    "unnormalize_layer(tf.constant([[[0,0]]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Obviamente, como pasamos como argumento el `0,0` recuperamos los valores medios de demanda y temperatura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Operando con una capa de One-Hot-Encoding\n",
    "\n",
    "Vamos a dar un ejemplo sobre cómo opera la capa de `tf.keras.layers.StringLookup` (disponible en versiones de TF recientes) buscando transformar el atributo `Tipo día` en atributos tipo one-hot.\n",
    "\n",
    "Analizamos cuántos registros hay por `Tipo día`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "day, idx, count = tf.unique_with_counts(tf.reshape(cat_train, [-1]))\n",
    "for a, b in zip(day, count):\n",
    "    print('Días: {} Cantidad: {}'.format(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Ahora probamos la capa para entender un poco más su operación. Creamos la capa informando que encontrará 7 tokens (uno por día de la semana),  sin *out-of-vocabulary* (aunque podría dejarse en un valor positivo para detectar errores). Forzamos la codificación tipo *one-hot*. Por default asigna un entero a cada término diferente. Luego se debe crear el vocabulario ejecutando el método `.adapt()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = len(day)\n",
    "\n",
    "lookup_layer = tf.keras.layers.StringLookup(\n",
    "    max_tokens=max_tokens,\n",
    "    num_oov_indices=0,\n",
    "    mask_token=None,\n",
    "    vocabulary=None,\n",
    "    # output_mode='int'\n",
    "    output_mode='one_hot'\n",
    ")\n",
    "\n",
    "lookup_layer.adapt(cat_train)\n",
    "lookup_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Vemos como transforma la serie con información categórica en 7 columnas de tipo entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_layer(cat_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Con el siguiente ejemplo confirmamos cómo transforma el atributo de `Tipo día` asignando un 1 a una de las 7 columnas correspondientes a cada día de la semana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_cat[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_layer('Domingo o Feriado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "El orden dado a las columnas es compartido con el orden del vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Construyendo una capa custom de preprocesado\n",
    "\n",
    "Resumidamente, la capa custom necesita definir un método `.call()` que se ejecutará dentro del modelo y un método `.adapt()` para configurar los parámetros de la capa custom. Previamente, inicializamos la capa `StringLookup` que nos permitirá transformar los valores categóricos en one-hot. Aquí proponemos una capa custom tipo one-hot para agrupar todos los días laborables en una misma clase. Proponemos, más adelante,  un ejercio que usa la capa `tf.keras.layers.StringLookup` sin modificar, de manera que asigna individualmente un one-hot a cada día de la semana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our definition for the categorical preporcossing layer\n",
    "class OneHotPre(keras.layers.Layer):\n",
    "    def __init__(self, index, max_tokens, vocabulary=None, num_oov_indices=0, **kwargs):\n",
    "        self._index = index\n",
    "        self._lookup_layer = tf.keras.layers.StringLookup(\n",
    "            max_tokens = max_tokens,\n",
    "            num_oov_indices = num_oov_indices,\n",
    "            vocabulary = vocabulary,\n",
    "            # output_mode = 'int'\n",
    "            output_mode = 'one_hot')\n",
    "        super().__init__(**kwargs)\n",
    "    def _to_day_type(self, value):\n",
    "        day_type = tf.strings.lower(value)\n",
    "        day_type = tf.strings.regex_replace(day_type, '^.*h.bil.*$', 'laborable')\n",
    "        day_type = tf.strings.regex_replace(day_type, '^.*semilaborable.*$', 'semilaborable')\n",
    "        return tf.strings.regex_replace(day_type, '^.*feriado.*$', 'nolaborable')\n",
    "    def adapt(self, data_sample):\n",
    "        self._lookup_layer.adapt(self._to_day_type(data_sample[:,:,self._index]))\n",
    "    def call(self, inputs):\n",
    "        return self._lookup_layer(self._to_day_type(inputs[:,:,self._index]))\n",
    "    def get_config(self):\n",
    "        return {'vocabulary': self._lookup_layer.get_vocabulary()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "La variable index permite seleccionar la columna a la que queremos aplicar one-hot (aquí es solo una columna). La cantidad de tokens es 3 ya que definimos 3 tipos de día: laborable, semilaborable, y no laborable.\n",
    "\n",
    "Debemos anticipar a las capas *custom* qué datos recibirán. El método `adapt()` prepara a la capa antes de ser usada por dentro del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "one_hot_layer = OneHotPre(index, 3)\n",
    "one_hot_layer.adapt(cat_train)\n",
    "one_hot_layer.get_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Definición del modelo usando la Functional API\n",
    "\n",
    "Functional API es otro método para definir un modelo de NN que permite abrir varios flujos de información habilitando tratamientos específicos en canales diferentes. Enfatiza la definición de entradas y salidas.\n",
    "\n",
    "En este caso, definimos el modelo de manera que procese la información de entrada por 2 canales diferentes. Los datos numéricos se normalizan, los categóricos pasan por la capa custom de codificación one-hot. Luego, ambas salidas se unen para continuar su procesamiento en la primer capa con celdas LSTM. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API Model \n",
    "numeric_input = keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]), dtype=tf.float32)\n",
    "normalized = normalize_layer(numeric_input)\n",
    "categorical_input = keras.layers.Input(shape=(cat_train.shape[1], cat_train.shape[2]), dtype=tf.string)\n",
    "encoded = one_hot_layer(categorical_input)\n",
    "concat = keras.layers.concatenate([normalized, encoded])\n",
    "lstm_lyr = keras.layers.LSTM(10)(concat)\n",
    "dropout_lyr = keras.layers.Dropout(0.2)(lstm_lyr)\n",
    "dense_lyr = keras.layers.Dense(1, activation='linear')(dropout_lyr)\n",
    "model_exo = keras.models.Model(inputs=[numeric_input, categorical_input], outputs=[dense_lyr])\n",
    "model_exo.compile(loss = 'MeanSquaredError', metrics=['MAE'], optimizer='Adam')\n",
    "model_exo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "El método `.summary()` nos muestra cómo fluirá la información entre las capas de NN y cuántos parámetros se deberán ajustar en cada una. Otra alternativa para representar la configuración de la NN es usar `tf.keras.utils.plot_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model_exo, to_file='Figs/model.png', rankdir=\"LR\", show_shapes=True,  show_layer_names=True)\n",
    "\n",
    "#tf.keras.utils.plot_model(multivariate_lstm, to_file='Figs/model.png', show_shapes=True,\n",
    "#                          show_dtype=True, show_layer_names=True, rankdir='TB',\n",
    "#                          expand_nested=True, dpi=200, show_layer_activations=True,\n",
    "#                          show_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Ajuste del modelo\n",
    "\n",
    "Entrenamos el modelo, es decir, ajustamos los pesos de acuerdo al esquema definido cuando fue compilado, por ejemplo optimizer='Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_target = keras.layers.Normalization(axis=1, mean=normalize_layer.mean[0,0,0],\n",
    "                                              variance=normalize_layer.variance[0,0,0])\n",
    "# normalize_target(tf.constant([[0]]))\n",
    "\n",
    "# Model training\n",
    "history = model_exo.fit([X_train, cat_train], [normalize_target(y_train)], epochs=30, validation_split=0.1)\n",
    "\n",
    "# early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "# history = model_exo.fit([X_train, cat_train], [y_train], epochs=3000, validation_split=0.1, callbacks=[early_stopping_cb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Para tener una idea sobre la convergencia del moelo, mostramos las curvas de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "# plt.gca().set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Luego que el modelo ha sido ajustado, es muy fácil obtener nuevas predicciones, como por ejemplo cuando le pasamos el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = model_exo.predict([X_test, cat_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Evaluación de los resultados\n",
    "\n",
    "Ahora vamos a representar nuestro resultado. Construimos un `DataFrame` para almacenar los valores de `DEMANDA TOTAL` predecida y observada, y su fecha (que recuperamos de los datos originales). Este `DataFrame` nos facilitará el cálculo de distintas métricas que califican el grado de acuerdo de las predicciones del modelo con los valores registrados. Asimismo, representaremos en una figura ambas series de datos para tener una estimación visual del desempeño del modelo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast Plot with Dates on X-axis\n",
    "LSTM_eval = pd.DataFrame({\n",
    "    'Predicted_DEMANDA': predicted_values[:, 0],\n",
    "    'Actual_DEMANDA': normalize_target(y_test)[:, 0],\n",
    "})\n",
    "\n",
    "LSTM_eval.loc[:, 'Date'] = dataFrame['Fecha'][-len(y_test):].values\n",
    "# LSTM_eval.set_index('Date', inplace=True)\n",
    "LSTM_eval['Date'] = pd.to_datetime(LSTM_eval['Date'])\n",
    "LSTM_eval.set_index('Date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una figura para representar las series de datos\n",
    "def FigPredActual(d, title):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    #  highlight the  forecast\n",
    "    # highlight_start = int(len(d) * 0.9)  \n",
    "    # highlight_end = len(d) - 1  # Adjusted to stay within bounds\n",
    "    # Plot the actual values\n",
    "    # plt.plot(d[['Actual_DEMANDA']][:highlight_start], label=['Actual_DEMANDA'])\n",
    "    plt.plot(d[['Actual_DEMANDA']], label=['Actual_DEMANDA'])\n",
    "    \n",
    "    # Plot predicted values with a dashed line\n",
    "    plt.plot(d[['Predicted_DEMANDA']], label=['Predicted_DEMANDA'], linestyle='--')\n",
    "    \n",
    "    # Highlight the forecasted portion with a different color\n",
    "    # plt.axvspan(d.index[highlight_start], d.index[highlight_end], facecolor='lightgreen', alpha=0.5, label='Forecast')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Dates')\n",
    "    plt.ylabel('Values')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Trazamos ambas series de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigPredActual(LSTM_eval, 'Multivariate Time-Series forecasting using LSTM and Exogenous variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "Definimos una función conveniente para calcular diferentes métrias que nos informan sobre el grado de acuerdo de las predicciones del modelo con las observaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def performance(d):\n",
    "    return {\n",
    "        'MSE': mean_squared_error(d['Actual_DEMANDA'].to_numpy(), d['Predicted_DEMANDA'].to_numpy()),\n",
    "        'MAE': mean_absolute_error(d['Actual_DEMANDA'].to_numpy(), d['Predicted_DEMANDA'].to_numpy()),\n",
    "        'R2': r2_score(d['Actual_DEMANDA'].to_numpy(), d['Predicted_DEMANDA'].to_numpy())\n",
    "    }\n",
    "\n",
    "performance(LSTM_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "**Preguntas**\n",
    "\n",
    "1. ¿Cómo se comparan las métricas luego de agregar las variables exógenas respecto al caso visto en la guía anterior que solo usa la variable `DEMANDA TOTAL` (o variable endógena)?\n",
    "2. ¿Sería conveniente usar `keras.callbacks.EarlyStopping`?\n",
    "3. ¿Se podría tomar la salida del valor pronosticado para incorporarlo a la serie de entrada de manera de pronosticar en más de un paso de tiempo adelante? Más en general, ¿Se podría dar un pronóstico para más de un paso de tiempo futuro? ¿Qué estrategia se podría usar?\n",
    "4. ¿Se podría sumar la transformación inversa como una capa final del modelo de manera de evitar la transformación del target? Realice una prueba. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Repetir el análisis pero usando `tf.keras.layers.StringLookup` sin modificar, de manera que asigne individualmente un one-hot a cada día de la semana. ¿Tiene un impacto positivo un análisis que consdiere individualemnte cada día de la semana? ¿Se puede saber si hay días específicos de mayor demanda? ¿Se puede estimar la incertidumbre en esa afirmación? \n",
    "2. Agregar otras variables exógenas o atributos que representen el paso del tiempo (ej. `pd.Timestamp.timestamp`) como los ciclos del año (funciones armónicas). Evaluar los resultados luego de incluir esta información adicional.\n",
    "3. Obtener una serie *baseline* a partir de un modelo `Dense` similar al analizado en la guía anterior pero que ahora también incorpore las variables exógenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.12-TF2.18",
   "language": "python",
   "name": "venv-tf2.18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
