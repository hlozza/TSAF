{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Guía 03 - Regresión con series temporales\n",
    "\n",
    "Continuaremos nuestro estudio de caso empleando los datos de demanda eléctrica diaria obtenidos del sitio de Cammesa. Ahora, evaluaremos dos modelos. El primero realizará sus predicciones en base a los valores registrados previamente de demanda eléctrica. El segundo sumará variables exógenas  (temperatura media y el tipo de jornada). Introducieremos el esquema de validación cruzada para luego hallar un conjunto de parámetros con los que el modelo logra la mejor respuesta. Esta mejor respuesta siempre requiere de algún tipo de función que compare y cuantifique el acuerdo entre los valores observados (valores en el paso/s siguiente/s o respuestas) y los predecidos por el modelo. Finalmente, se debe informar sobre la calidad del modelo teniendo presente que la evaluación deberá realizarse sobre un conjunto de datos independientes de los que participaron en la calibración.\n",
    "\n",
    "## Parte A - Modelo de regresión que solo usa los valores registrados previamente\n",
    "\n",
    "Nos proponemos realizar predicciones para la demanda eléctrica total (`DEMANDA TOTAL`) tomando solamente los valores registrados en fechas previas a la que se busca pronosticar. La idea detrás de este abordaje es que la predicción en un tiempo futuro es posible con la información disponible de las lecturas previas. Es interesante notar que, si bien la predicción es sobre la misma variable observada, no hay filtración de datos (leak) porque solo se consideran muestras de tiempos anteriores al que se busca pronosticar.\n",
    "\n",
    "Comenzamos cargando y preparando los datos de `'Base Demanda Diaria 2017 2025.xlsx`. Renombramos algunas columnas para mayor comodidad y agregamos un índice a partir del atributo `Fecha` para faciliar el cálculo de los días transcurridos desde el inicio de la serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "baseDiaria = pd.read_excel('Data/Base Demanda Diaria 2017 2025.xlsx',\n",
    "                           sheet_name='Datos Región', skiprows=4)\n",
    "\n",
    "baseDiaria.head()\n",
    "\n",
    "# Simplificamos la DB\n",
    "baseDiaria['Jornada'] = baseDiaria[['Tipo día']].replace(to_replace=['.*abiles', 'Sabado.*',\n",
    "                                             'Domingo.*'],\n",
    "                                 value=['Hábiles', 'Sábado', 'Domingo'],\n",
    "                                 regex=True)\n",
    "\n",
    "\n",
    "baseDiaria.set_index(pd.to_datetime(baseDiaria['Fecha']),inplace=True)\n",
    "baseDiaria = baseDiaria[['DEMANDA TOTAL', 'Jornada', 'TEMPERATURA REFERENCIA MEDIA GBA °C']]\n",
    "baseDiaria.rename(columns={'TEMPERATURA REFERENCIA MEDIA GBA °C':'Tmedia'},inplace=True)\n",
    "baseDiaria.rename(columns={'DEMANDA TOTAL':'Dtotal'},inplace=True)\n",
    "\n",
    "baseDiaria['Interval'] = np.float32((baseDiaria.index - baseDiaria.index[0]).days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Al usar el método `asfreq()` de `Pandas`, cualquier espacio vacío en la serie temporal se rellenará con valores `np.nan` para que coincidan con la frecuencia especificada. Por lo tanto, es fundamental verificar si hay valores faltantes después de esta transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDiaria.asfreq('D')\n",
    "print(f'Number of rows with missing values: {baseDiaria.isnull().any().sum()}')\n",
    "print(f'Number of rows with NAN values: {baseDiaria.isna().any().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Construiremos nuestro conjunto de datos tomando diferntes *lags* sobre el atributo `DEMANDA TOTAL`. Los lags son corrimientos de las series de manera que al combinarlas nos permiten leer los valores de pasos de tiempo anteriores dentro de una sola fila. Inicialmente, tomamos los datos de una semana atrás para pronosticar el valor del día siguiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasos = 7\n",
    "\n",
    "X = baseDiaria[['Dtotal']]\n",
    "X = X.rename(columns={X.columns[-1]:'Lag_0'})\n",
    "for i in range(1,pasos+1):\n",
    "    new_series = baseDiaria['Dtotal'].shift(i)\n",
    "    X = pd.concat([X, new_series], axis=1)\n",
    "    X = X.rename(columns={X.columns[-1]:'Lag_' + str(i)})\n",
    "\n",
    "X.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Descartamos las primeras filas por efecto del corrimento. Notamos que también podríamos realizar corrimientos en sentido opuesto para generar series de datos adelantadas, lo que conduciría a proponer targets avanzados en más de un paso de tiempo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[pasos:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Separación en datos de entrenamiento y prueba\n",
    "\n",
    "El orden temporal es lo que caracteriza a una serie de tiempo. Por tanto, al particionar en datos de entrenamiento y prueba debemos evitar cambiar el orden. El mecanismo de slice permite fácilmente satisfacer este criterio\n",
    "\n",
    "    ss = int(0.8 * X.shape[0])\n",
    "    X_train = X[:-ss]\n",
    "    X_test = X[ss:]\n",
    "\n",
    "### Validación cruzada\n",
    "\n",
    "Aprovechamos para introducir la idea de validación cruzada. Dado que no se pueden mezclar las carpetas, se opta por agregar, sucecesivamente, una carpeta al conjunto de entrenamiento en cada iteración. La carpeta siguiente, que aún no ha sido empleada, se reserva como conjunto de validación.\n",
    "\n",
    "![Validación cruzada de series temporales](Figs/time_series_cv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Ajuste y evaluación de desempeño del primer modelo\n",
    "\n",
    "Nos valemos del método `split()` de la clase `TimeSeriesSplit` de la biblioteca `Scikit-learn` para failitar la obtención de los índices. Probamos con una regresión lineal... Luego, calculamos la media y desviación estándard para la métrica propuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "print(tscv)\n",
    "\n",
    "lr = LinearRegression()\n",
    "train_rmse = np.array([])\n",
    "test_rmse = np.array([])\n",
    "for train, test in tscv.split(X):\n",
    "    X_train = X.iloc[train,:]\n",
    "    y_train = X_train.pop('Lag_0')\n",
    "    X_test = X.iloc[test,:]\n",
    "    y_test = X_test.pop('Lag_0')\n",
    "\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_train)\n",
    "    train_rmse = np.append(train_rmse, root_mean_squared_error(y_train, y_pred))\n",
    "\n",
    "    y_pred = lr.predict(X_test)\n",
    "    test_rmse = np.append(test_rmse, root_mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print('Train RMSE')\n",
    "print(f'Mean: ({np.mean(train_rmse):.0f} +/- {np.std(train_rmse):.0f})MW')\n",
    "print('Validation RMSE')\n",
    "print(f'Mean: ({np.mean(test_rmse):.0f} +/- {np.std(test_rmse):.0f})MW')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Notamos que este modelo logra un desempeño mejor que los *benchmarks* propuestos en la guía 0.\n",
    "\n",
    "Comparamos la salida del modelo para datos de prueba con las lecturas registradas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy =pd.DataFrame(y_pred, index=y_test.index)\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "# 2. Plot the data\n",
    "plt.plot(yy, label='Pred.')\n",
    "plt.plot(y_test, '.', label='Obs.')\n",
    "\n",
    "# 3. Format the x-axis to show only years\n",
    "# Use YearLocator to place ticks at the start of each year\n",
    "plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "\n",
    "# Use DateFormatter to format the labels as a 4-digit year (%Y)\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "# 4. Optional: improve readability by rotating labels\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "\n",
    "plt.ylabel('DEMANDA TOTAL (MW)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Parte A - Ejercicios \n",
    "\n",
    "1. Representar la función de autocorrelación para la serie de residuos (en entrenamiento y validación). ¿Se asemejan los residuos a un error gaussiano? ¿Por qué? \n",
    "2. Analizar el peso de los coeficientes para cada uno de los 7 días anteriores considerados en el modelo de regresión lineal.\n",
    "3. Modificar la cantidad de pasos o días observados y evaluar el desempeño del modelo en función de la cantidad de pasos. \n",
    "4. Reemplazar el ajuste lineal por un modelo de regresión no-lineal como Random Forest, XGBoost, u otro. Comparar el desempeño con el primer modelo.\n",
    "5. Proponer un modelo que suma un pronóstico para 2 pasos adelante en el tiempo. ¿Se confirma que la confianza en la predicción decae a medida que se aleja el horizonte de pronóstico?\n",
    "6. Realizar un pronóstico para 2 pasos adelante mediante recur \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Parte B - Un ejemplo más completo con optimización de hiper-parámetros\n",
    "\n",
    "Como lo adelantamos, buscamos establecer un mejor modelo para la predicción de la demanda eléctrica que no solo tenga en cuenta el pasado sino, también, los efectos de la temperatura, del tipo de jornada y el crecimiento de la demanda con los años. \n",
    "\n",
    "\n",
    "Prepararemos nuestro conjunto de datos tomando diferntes lags sobre el atributo `DEMANDA TOTAL`. Esta vez, constrtruiremos un `transformer` de `sklearn` para que dentro del preprocesado podamos variar la cantidad de pasos previos de `DEMANDA TOTAL` incluídos. Más adelante, veremos paquetes que extienden las funcionalidades de `sklearn` para trabajar con series de tiempo  como `skForecast` o `feature_engine.timeseries.forecasting` que incluye `LagFeatures()`, o `sktime.transformations.series.lag` con el método `Lag()` que automatiza la creación de atributos desplazados en el tiempo.   Además, incluíremos las variables explicativas adicionales mencionadas anteriormente.\n",
    "\n",
    "### Automatiazción de la creación de atributos desplazados en el tiempo\n",
    "\n",
    "Buscamos construir un `Transformer` de `sklearn` que nos permita incorporar en el preprocesado la creación de atributos con diferentes *lags* a partir de una serie de datos seleccionada. `sklearn` ofrece el `BaseEstimator` y el `TransformerMixin` que facilitan su creación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class LagFeatures(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, lags=1):\n",
    "        self.lags = lags\n",
    "    def fit(self, X, y=None):\n",
    "        # Compute mean for handling potential NANs created by shifting\n",
    "        self.X_mean = X.mean() \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_lagged = pd.DataFrame([])\n",
    "        cols_to_lag = X.columns\n",
    "        for col in cols_to_lag:\n",
    "            for lag in range(1, self.lags+1):\n",
    "                fill_value = self.X_mean[col]\n",
    "                new_series = X[col].shift(lag, fill_value=fill_value)\n",
    "                X_lagged = pd.concat([X_lagged, new_series], axis=1)\n",
    "                X_lagged = X_lagged.rename(columns={X_lagged.columns[-1]:col + '_Lag_' + str(lag)})\n",
    "        return X_lagged\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Es importante notar que debemos rellenar (*imputar*) los valores faltantes que resultan del desplazamiento con el promedio de la columna. `sklearn` no permite transformar simultáneamente el *target* y recortar el conjunto de datos para comenzar la serie con valores de pasos anteriores en todos los *lags*. [Scikit-Learn Pipeline Transformers — The hassle of transforming target variables (Part 1)](https://medium.com/analytics-vidhya/scikit-learn-pipeline-transformers-the-hassle-of-transforming-target-variables-part-1-6dfb714e2aad)\n",
    "\n",
    "Hacemos una prueba de cómo opera el método `fit_transform` de la clase recién creada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_transformer = LagFeatures()\n",
    "\n",
    "lag_transformer.set_params(lags=7)\n",
    "print(f'Lags: {lag_transformer.get_params()}')\n",
    "X = lag_transformer.fit_transform(baseDiaria[['Dtotal']])\n",
    "\n",
    "X = pd.concat([X, baseDiaria], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Variables dummies\n",
    "\n",
    "Una situación frecuente, y presente en nuestro estudio de caso, es una en que debemos distinguir entre días hábiles, sábados, o domingos y feriados. Algunas veces, estas variables categóricas se denominan variables *dummies*. La manera habitual de tratar estos datos es asignar un 1 o un 0 para la presencia o ausencia de dicha característica: \n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n X_n + \\gamma_1 d_1 + \\cdots \\gamma_m d_m\n",
    "$$\n",
    "\n",
    "Así, las variables $x_i$ pueden referir a las distintas potencias de la `Tmedia` o los *lags*. Además, las variables $d_1, d_2, d_3$ representarían los días hábiles, sábados o domingos y feriados. Por ejemplo, un día hábil se codifica como $1, 0, 0$. Esto permite ajustar la serie completa de datos sin tener que partirla y ajustarla individualmente para cada tipo de día. Esta [codificación](https://scikit-learn.org/stable/modules/preprocessing.html) se logra fácilmente con la ayuda de clase `OneHotEncoder` de la biblioteca scikit-learn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Transformaciones por columnas y procesamientos en pipelines \n",
    "\n",
    "Como es habitual encontrar datos que reúnen tanto valores numéricos como categóricos, la biblioteca scikit-learn ofrece herramientas para facilitar el [preprocesamiento](https://scikit-learn.org/stable/modules/preprocessing.html) como las transfromaciones por columnas `ColumnTransformer`. Asimismo, la biblioteca ofrece los `Pipeline` para [automatizar una cadena de procesos](https://scikit-learn.org/stable/modules/compose.html) que se repiten sobre distintos conjuntos se datos o, alternativamente, que se repiten para diferentes juegos de parámetros. La transformación por columnas nos permitirá combinar los desplazamientos en el tiempo, las potencias y las variables dummies... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operations by columns\n",
    "num_attribs = ['Tmedia']\n",
    "cat_attribs = ['Jornada']\n",
    "lag_attribs = ['Dtotal']\n",
    "pass_attribs = ['Interval']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", PolynomialFeatures(), num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    (\"lag\", LagFeatures(), lag_attribs),\n",
    "    (\"pass\", 'passthrough', pass_attribs),\n",
    "    # Default drop para columnas no especificadas\n",
    "    # passthrough: Las columnas pasan al siguiente paso sin ninguna transformación\n",
    "])\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression()),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Vemos dos tipos de especiales de estimadores: *drop* y *passthrough*. El primero impide que las columnas listadas pasen al proceso siguiente. El otro permite que las columnas listadas pasen al paso siguiente sin ninguna trasnformación adicional.\n",
    "\n",
    "Es importante conocer el nombre de los distintos parámetros de  manera de poder pasar valores para que las sucesivas corridas de prueba se puedan ejecutar con configuraciones diferentes. Para esto ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_pipeline.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Con la informacióna anterior, podemos establecer una configuración conveniente para el proceso de potenciación `PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlags = 7\n",
    "degree = 2\n",
    "full_pipeline.set_params(preprocessor__num__degree=degree,\n",
    "                         preprocessor__num__include_bias=False,\n",
    "                         preprocessor__cat__drop='first',\n",
    "                         preprocessor__lag__lags=nlags\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Flujo de trabajo con validación cruzada (CV) y optimización de hiper-parámetros\n",
    "\n",
    "Calibrar los parámetros de una función de predicción y probarla con los mismos datos es un error metodológico: un modelo que simplemente repetiría las etiquetas de las muestras que acaba de ver tendría una puntuación perfecta pero no predeciría nada útil todavía de los datos no vistos. Esta situación se llama sobreajuste. Para evitarlo, es una práctica común al realizar un experimento de aprendizaje automático (supervisado) mantener parte de los datos disponibles como un conjunto de prueba `X_test`, `y_test`. A continuación se muestra un diagrama de flujo de trabajo típico de [validación cruzada](https://scikit-learn.org/0.22/modules/cross_validation.html#time-series-split) en el entrenamiento de modelos. Los mejores parámetros pueden determinarse mediante técnicas de búsqueda en una grilla.\n",
    "\n",
    "<img src=\"Figs/grid_search_workflow.png\" width=\"300\" alt=\"Diagrama de flujo del proceso de calibración/validación\">\n",
    "\n",
    "\n",
    "Recientemente, presentamos el procedimiento de validación cruzada con series temporales. En el enfoque básico, el conjunto de entrenamiento se divide en k+1 conjuntos más pequeños (existen otros enfoques pero generalmente siguen los mismos principios). Se entrena el modelo con los datos de la primeras carpetas y se valida con los datos en la carpeta inmediatamente siguiente. Luego, se repite para cada una de las k carpetas los siguientes pasos:\n",
    "\n",
    "1. Se toma k=1\n",
    "2. El modelo se entrena usando los k-folds primeros;\n",
    "3. El modelo resultante se valida con los datos en el k+1 fold.\n",
    "4. Mientras k es menor que la cantidad máxima de carpetas, se incremnta k y se retorna al paso 1.\n",
    "\n",
    "La medida de rendimiento informada por la validación cruzada k veces es entonces el promedio de los valores calculados en el lazo. Este enfoque puede ser costoso desde el punto de vista computacional, pero no desperdicia demasiados datos (como es el caso cuando se fija un conjunto de validación arbitrario), lo cual es una gran ventaja en problemas como la inferencia inversa donde el número de muestras es muy pequeño.\n",
    "\n",
    "Emplearemos al igual que en el caso anterior, el método `StratifiedShuffleSplit` para simplifcar el proceso de CV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Ajuste y evaluación de desempeño agregando variables exógenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "# print(tscv)\n",
    "\n",
    "train_rmse = np.array([])\n",
    "test_rmse = np.array([])\n",
    "for train, test in tscv.split(baseDiaria[nlags:]):\n",
    "    X_train = baseDiaria.iloc[train,:]\n",
    "    y_train = baseDiaria['Dtotal'].iloc[train]\n",
    "    X_test = baseDiaria.iloc[test,:]\n",
    "    y_test = baseDiaria['Dtotal'].iloc[test]\n",
    "    # No hay leak, en el preprocesado se pierde la columna Dtotal \n",
    "    \n",
    "    X_train_full = full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = full_pipeline.predict(X_train)\n",
    "    train_rmse = np.append(train_rmse, root_mean_squared_error(y_train, y_pred))\n",
    "\n",
    "    y_pred = full_pipeline.predict(X_test)\n",
    "    test_rmse = np.append(test_rmse, root_mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f'Modelo con {nlags} lags y grado {degree}')\n",
    "print('Train RMSE')\n",
    "print(f'Mean: ({np.mean(train_rmse):.0f} +/- {np.std(train_rmse):.0f})MW')\n",
    "print('Validation RMSE')\n",
    "print(f'Mean: ({np.mean(test_rmse):.0f} +/- {np.std(test_rmse):.0f})MW')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Representación de la dependencia con la temperatura y el día de la semana\n",
    "\n",
    "Vamos a representar el comportamiento de la demanda para cada tipo de jornada asumiendo un día fijo (el último de la serie). (Ya vimos que hay una tendencia al incremento de la demanda con los años de aproximadamente el 1%. Este tipo de gráficos nos permite investigar *¿Qué pasaría si..?*. Por ejemplo, podemos responder a qué pasaría si, bajo las mismas condiciones de demanda y temperatura, se trata de un día hábil o feriado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot en función de jornada\n",
    "# Fijo para una Fecha\n",
    "\n",
    "jornadas_col = list(map(lambda x: plt.cm.tab20c.colors[['Hábiles', 'Sábado', 'Domingo'].index(x)], baseDiaria['Jornada']))\n",
    "plt.scatter(baseDiaria['Tmedia'], baseDiaria['Dtotal'], c=jornadas_col)\n",
    "\n",
    "\n",
    "for i, sdh in enumerate(['Hábiles', 'Sábado', 'Domingo']):\n",
    "\n",
    "    fig_df = pd.DataFrame()\n",
    "    for tmedia in np.linspace(5, 35, 100):\n",
    "    \n",
    "        fig_data = baseDiaria.iloc[-nlags-1:]\n",
    "        fig_data.loc[:,'Jornada'] = sdh\n",
    "        fig_data.loc[:,'Tmedia'] = tmedia  \n",
    "        fig_df = pd.concat([fig_df, fig_data]) \n",
    "        \n",
    "    fig_df_predict = full_pipeline.predict(fig_df)[nlags::nlags+1]\n",
    "    print(sdh, ' media ', np.mean(fig_df_predict), '\\n')\n",
    "    plt.plot(fig_df['Tmedia'][nlags::nlags+1], fig_df_predict,\n",
    "                color=plt.cm.tab20b.colors[i],\n",
    "                label='Fecha {} {}'.format(baseDiaria.index[-1], sdh))\n",
    "\n",
    "plt.xlabel('Tmedia (°C)')\n",
    "plt.ylabel('DEMANDA TOTAL (MW)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Búsqueda de mejores hiper-parámetros sobre una grilla de valores \n",
    "\n",
    "Una vez que automatizamos la cadena de procesamiento, podemos ejecutarla para diferentes conjuntos de parámteros. En este caso, vamos a variar el grado máximo del ajuste polinómico (En el ejemplo anterior lo habíamos fijado en 2). La intención es encontrar el grado que produce el mejor puntaje en el proceso de validación cruzada. `GridSearchCV` repite la ejecución de la misma cadena de procesamiento para todos los valores que se le pasen como argumento en formato diccionario. Como la función busca el puntaje más alto, en el caso de que se evalúe el acuerdo mediante el RMSE, se debe pasar su inverso `neg_root_mean_squared_error`. \n",
    "\n",
    "En este caso particular, también le pasamos a `GridSearchCV` la estrategia de partición en los difrentes compartimientos ya que, por default emplea `k-fold`. \n",
    "\n",
    "#### Búsqueda de mejor grado para atributos polinomiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "# print(tscv)\n",
    "\n",
    "parameters = {'preprocessor__num__degree': np.arange(2,11).tolist()}\n",
    "\n",
    "grid = GridSearchCV(full_pipeline, parameters,\n",
    "                    scoring='neg_root_mean_squared_error',\n",
    "                    cv=tscv, return_train_score=True)\n",
    "\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# grid = RandomizedSearchCV(full_pipeline, parameters, \n",
    "#                           scoring='neg_root_mean_squared_error',\n",
    "#                           cv=tscv, return_train_score=True, random_state=0)\n",
    "\n",
    "# When the Pipeline is printed out in a jupyter notebook an HTML\n",
    "# representation of the estimator is displayed:\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Luego de la búsqueda imprimimos el conjunto de mejores parámetros. El mejor resultado se obtiene con un polinomio de grado 6. Tmbién observamos que el RMSE es negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling ‘fit’ triggers the cross-validated search for\n",
    "# the best hyper-parameters combination:             \n",
    "grid.fit(baseDiaria, baseDiaria['Dtotal'])\n",
    "\n",
    "print(\"Best params:\")\n",
    "print(grid.best_params_)\n",
    "\n",
    "# The internal cross-validation scores obtained by those parameters is:\n",
    "\n",
    "print(f\"Internal CV score: {grid.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "En general, los resultados de cada iteración se presentan en la variable `grid.results`. Nos enfocamos solamente en las columnas de puntaje para los datos de prueba y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also introspect the top grid search results as a pandas dataframe:\n",
    "\n",
    "grid_results = pd.DataFrame(grid.cv_results_)\n",
    "# print('CV Results Column Names: ', grid_results.columns)\n",
    "\n",
    "grid_results.rename(columns={'param_preprocessor__num__degree': 'degree'},\n",
    "                    inplace=True)\n",
    "grid_results[['mean_train_score',\n",
    "              'mean_test_score']] = grid_results[['mean_train_score',\n",
    "                                'mean_test_score']].apply(lambda x: x * (-1))\n",
    "print(grid_results[['degree', 'mean_train_score',\n",
    "                    'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values(by='rank_test_score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Asimismo, tomamos los resultados de la variable `grid.results` para mostrar como los datos de entreamiento siempre producen un menor RMSE respecto a los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = grid_results[['degree', 'mean_train_score',\n",
    "                    'mean_test_score']].melt('degree',\n",
    "                                             var_name='cols', value_name='RMSE')\n",
    "# Apply the default theme\n",
    "sns.set_theme()\n",
    "sns.pointplot(\n",
    "    data=dfm,\n",
    "    x='degree', y='RMSE', hue='cols' \n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Se aprecia el mínimo de RMSE en 6 pero un polinomio de grado 4 ya lo hace muy bien.\n",
    "\n",
    "#### Búsqueda simultánea de mejor grado para atributos polinomiales y lags\n",
    "\n",
    "Finalmente, vamos a variar smilutáneamente el orden del polígono en la temperatura media y el número de lags buscando el mejor estimador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'preprocessor__num__degree': np.arange(2,11).tolist(),\n",
    "             'preprocessor__lag__lags': np.arange(5,15).tolist()}\n",
    "\n",
    "grid = GridSearchCV(full_pipeline, parameters,\n",
    "                    scoring='neg_root_mean_squared_error',\n",
    "                    cv=tscv, return_train_score=True)\n",
    "\n",
    "# Calling ‘fit’ triggers the cross-validated search for\n",
    "# the best hyper-parameters combination:             \n",
    "grid.fit(baseDiaria, baseDiaria['Dtotal'])\n",
    "\n",
    "print(\"Best params:\")\n",
    "print(grid.best_params_)\n",
    "\n",
    "# The internal cross-validation scores obtained by those parameters is:\n",
    "\n",
    "print(f\"Internal CV score: {grid.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also introspect the top grid search results as a pandas dataframe:\n",
    "\n",
    "grid_results = pd.DataFrame(grid.cv_results_)\n",
    "# print('CV Results Column Names: ', grid_results.columns)\n",
    "\n",
    "grid_results.rename(columns={'param_preprocessor__num__degree': 'degree', 'param_preprocessor__lag__lags': 'lags'},\n",
    "                    inplace=True)\n",
    "grid_results[['mean_train_score',\n",
    "              'mean_test_score']] = grid_results[['mean_train_score',\n",
    "                                'mean_test_score']].apply(lambda x: x * (-1))\n",
    "print(grid_results[['rank_test_score','degree', 'lags', 'mean_train_score',\n",
    "                    'mean_test_score']].sort_values(by='rank_test_score'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Vemos que duplicando la cantidad de lags (14) no mejora sustancialmente el desempeño en validación respecto al caso de considerar solo una semana de valores anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Parte B - Ejercicios\n",
    "\n",
    "1. Representar la función de autocorrelación para la serie de residuos (en entrenamiento y validación). ¿Se asemejan los residuos a un error gaussiano? ¿Por qué?  \n",
    "2. Evaluar el desempeño del modelo que solo usa variables exógenas (no tiene en cuenta los valores recientes de la `DEMANDA TOTAL`).  \n",
    "    a. ¿Se puede en este caso realizar mezcla de datos (shuffle)? ¿Se puede considerar este modelo como un *benchmark* para las técnicas de modelado de series temporales?  \n",
    "    b. Representar la función de autocorrelación para la serie de residuos.  \n",
    "    c. ¿Podría abordar a la serie de residuos como una serie temporal y buscar un modelo que mejor capture su cambio?  \n",
    "3. ¿Se puede mejorar el desempeño si en lugar de considerar 3 variables dummies por tipo de día se tomaran 7 variables, una por cada día de la semana?  \n",
    "4. Suponer que no se cuenta información de la temperatura media. Incorporar al modelo funciones trigonométricas para capturar los cambios de estaciones (y de consumo) a lo largo del año. ¿Cómo es el desempeño del modelo?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Parte C - Simplificando las predicciones y pronósticos con Skforecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Ejemplo de pronóstico a 14 días\n",
    "\n",
    "Vamos a dar un primer ejemplo sobre cómo emplear `Skforecast`. Tomamos los últimos 120 días de datos `DEMANDA TOTAL` y separamos los 14 días finales como conjunto de prueba para evaluar, posteriormente, la capacidad predictiva del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train-test\n",
    "# ==============================================================================\n",
    "steps = 14\n",
    "data_train = baseDiaria[-120:-steps].asfreq('D')\n",
    "data_test  = baseDiaria[-steps:].asfreq('D')\n",
    "print(\n",
    "    f\"Train dates : {data_train.index.min()} --- \"\n",
    "    f\"{data_train.index.max()}  (n={len(data_train)})\"\n",
    ")\n",
    "print(\n",
    "    f\"Test dates  : {data_test.index.min()} --- \"\n",
    "    f\"{data_test.index.max()}  (n={len(data_test)})\"\n",
    ")\n",
    "\n",
    "from skforecast.plot import set_dark_theme\n",
    "# set_dark_theme()\n",
    "fig, ax = plt.subplots(figsize=(6, 2.5))\n",
    "data_train['Dtotal'].plot(ax=ax, label='train')\n",
    "data_test['Dtotal'].plot(ax=ax, label='test')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Pronóstico recursivo multipaso\n",
    "\n",
    "Normalmente, se busca el mejor acuerdo entre la variable en el tiempo $t$ y las predicciones del modelo entrenado con n valores que atrasan  (n lags) respecto a ese tiempo. Luego, se puede realimentar el valor predecido en la serie y pronosticar a la variable en el tiempo $t+1$ originando el *pronóstico recursivo multipaso*. Otra opción es el *pronóstico directo* en el que se entrenan diferentes modelos para ajustar los valores futuros de la variable en $t, t+1, \\cdots$. Esta estrategia demanda mayor esfuerzo de cómputo ya que requiere entrenar tantos modelos como pasos futuros se planteen.     \n",
    "\n",
    "\n",
    "![Diagrama de proceso de predicción recursiva de múltiples pasos para predecir 3 pasos en el futuro utilizando los últimos 4 rezagos de la serie como predictores.](Figs/diagram-recursive-mutistep-forecasting.png)\n",
    "\n",
    "Diagrama de proceso de predicción recursiva de múltiples pasos para predecir 3 pasos en el futuro utilizando los últimos 4 lags de la serie como predictores.\n",
    "\n",
    "La clase `ForecasterRecursive` crea y entrena un modelo de pronóstico recursivo utilizando de base un estimador de `sklearn`(por ejemplo, nosotros elegimos `LinearRegression()`). Para el ejemplo, planteamos una ventana de 7 lags. Esto significa que el modelo utiliza los 7 registros anteriores al instante que se busca predecir o pronosticar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train forecaster\n",
    "# ==============================================================================\n",
    "from skforecast.recursive import ForecasterRecursive\n",
    "forecaster = ForecasterRecursive(\n",
    "                 estimator = LinearRegression(),\n",
    "                 lags      = 7\n",
    "             )\n",
    "\n",
    "forecaster.fit(y=data_train['Dtotal'])\n",
    "forecaster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### Ejemplo de predicción a 14 días\n",
    "\n",
    "Una vez entrenado el modelo, se predicen los datos de prueba (con un horizonte de 14 días)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "# ==============================================================================\n",
    "steps = 14\n",
    "predictions = forecaster.predict(steps=steps)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions versus test data\n",
    "# ==============================================================================\n",
    "fig, ax = plt.subplots(figsize=(6, 2.5))\n",
    "data_train['Dtotal'].plot(ax=ax, label='train')\n",
    "data_test['Dtotal'].plot(ax=ax, label='test')\n",
    "predictions.plot(ax=ax, label='predictions')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Backtesting\n",
    "\n",
    "El proceso de *backtesting* consiste en evaluar el rendimiento de un modelo predictivo aplicándolo retrospectivamente a datos históricos. Por lo tanto, se trata de un tipo especial de *validación cruzada* que se aplica al período o períodos anteriores.\n",
    "\n",
    "El propósito del backtesting es evaluar la precisión y la eficacia de un modelo e identificar posibles problemas o áreas de mejora.\n",
    "\n",
    "Se puede implementar *backtesting* utilizando una variedad de técnicas:\n",
    "* Partición en datos de entrenamiento y prueba (*simple train-test split*)\n",
    "* Backtesting without refit\n",
    "  ![backtesting without refit](Figs/time-series-backtesting-forecasting-no-refit.gif)\n",
    "* Backtesting with refit and increasing training size\n",
    "  ![backtesting with refit and increasing training size](Figs/time-series-backtesting-forecasting-with-refit.gif)\n",
    "* Backtesting with refit and fixed training size\n",
    "  ![backtesting with refit and fixed training size](Figs/time-series-backtesting-forecasting-refit-fixed-train-size.gif)\n",
    "* Backtesting with intermittent refit\n",
    "  ![backtesting with intermittent refit](Figs/time-series-backtesting-forecasting-intermittent-refit.gif)\n",
    "* Backtesting including gap, Backtesting with fold stride, otros\n",
    "\n",
    "Fuente:  Skforecast: time series forecasting with Python, Machine Learning and Scikit-learn by Joaquín Amat Rodrigo and Javier Escobar Ortiz, available under Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED) at https://cienciadedatos.net/documentos/py27-time-series-forecasting-python-scikitlearn.html\n",
    "\n",
    "Un ejemplo de backtesting con refit consta de los siguientes pasos (asumimos como ejemplo steps=10):\n",
    "\n",
    "1. Entrenar el modelo utilizando un conjunto de entrenamiento inicial, cuya longitud se especifica mediante `initial_train_size`.\n",
    "2. Una vez entrenado el modelo, se utiliza para realizar predicciones para los siguientes 10 pasos (steps). Estas predicciones se guardan para su posterior evaluación.\n",
    "3. Cuando `refit=True o int`, el tamaño del conjunto de entrenamiento aumenta agregando los 10 pasos anteriores (o 10 x n, donde n es el entero dado como argumento de `refit`), mientras que los siguientes 10 pasos se utilizan como datos de prueba.\n",
    "4. Después de ampliar el conjunto de entrenamiento, el modelo se vuelve a entrenar utilizando los datos de entrenamiento actualizados y luego se utiliza para predecir los próximos 10 pasos.\n",
    "5. Se repiten los pasos 3 y 4 hasta que se haya probado toda la serie.\n",
    "\n",
    "### Implementación de backtesting con TimeSeriesFold\n",
    "\n",
    "La clase `TimeSeriesFold` está diseñada para generar las particiones utilizadas en el proceso de backtesting para entrenar y evaluar el modelo. Al aprovechar sus diversos argumentos, ofrece una gran flexibilidad, lo que permite la simulación de escenarios como reajuste, no reajuste, origen variable y otros. El método `split` devuelve las posiciones de índice de las series temporales correspondientes a cada partición. Cuando se especifica `as_pandas=True`, la salida es un DataFrame con información detallada, incluyendo nombres de columnas descriptivos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### Ejemplo de evaluación por backtesting a 14 días con reajuste de parámetros\n",
    "\n",
    "Proponemos evaluar el modelo de pronóstico a 14 días utilizando la técnica de backtesting. Particularmente, elegimos backtesting con reajuste intermitente que resulta en un esquema similar al visto con `TimeSeriesSplit` de `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skforecast.model_selection import TimeSeriesFold, grid_search_forecaster, backtesting_forecaster\n",
    "\n",
    "# Backtesting forecaster\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterRecursive(\n",
    "                 estimator       = LinearRegression(),\n",
    "                 lags            = 7\n",
    "             )\n",
    "\n",
    "end_train = data_train.index[int(0.33 * len(data_train['Dtotal']))]\n",
    "cv = TimeSeriesFold(\n",
    "        steps                 = 14,\n",
    "        initial_train_size    = end_train,\n",
    "        refit                 = 2,\n",
    "        fixed_train_size      = False,\n",
    "        gap                   = 0,\n",
    "        allow_incomplete_fold = True\n",
    "     )\n",
    "\n",
    "metric, predictions = backtesting_forecaster(\n",
    "                          forecaster    = forecaster,\n",
    "                          y             = data_train['Dtotal'],\n",
    "                          cv            = cv,\n",
    "                          metric        = 'mean_squared_error',\n",
    "                          n_jobs        = 'auto',\n",
    "                          verbose       = True,\n",
    "                          show_progress = True\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest error and predictions\n",
    "# ==============================================================================\n",
    "print(f'RMSE: {np.sqrt(metric['mean_squared_error'][0]):.0f}MW')\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "# ==============================================================================\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "data_train.loc[end_train:, 'Dtotal'].plot(ax=ax)\n",
    "predictions['pred'].plot(ax=ax)\n",
    "ax.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Ajustes de hiper-parámetros\n",
    "\n",
    "`Skforecast` ofrece varias estrategias de búsqueda para encontrar la mejor combinación de hiperparámetros y lags. En este caso, se utiliza la función `grid_search_forecaster`, que compara los resultados obtenidos con cada combinación de hiperparámetros y lags e identifica la mejor.\n",
    "\n",
    "La búsqueda sobre una grilla evalúa una lista exhaustiva de combinaciones de hiperparámetros y lags para encontrar la configuración óptima de un modelo de pronóstico. Para realizar una búsqueda sobre una grilla con la biblioteca `skforecast`, se necesitan dos cuadrículas: una con diferentes lags (lags_grid) y otra con los hiperparámetros (param_grid).\n",
    "\n",
    "El proceso de búsqueda en la grilla implica los siguientes pasos:\n",
    "\n",
    "1. `grid_search_forecaster` reemplaza el argumento lags con la primera opción que aparece en lags_grid\n",
    "2. La función valida todas las combinaciones de hiperparámetros presentados en param_grid mediante *backtesting* o *validación de un paso adelante*.\n",
    "3. La función repite estos dos pasos hasta que haya evaluado todas las combinaciones posibles de lags e hiperparámetros.\n",
    "4. Si `return_best = True`, el pronosticador original se entrena con los mejores lags y la mejor configuración de hiperparámetros encontrados durante el proceso de búsqueda de la cuadrícula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### Ejemplo de ajuste de hiper-parámetros incorporando variables exógenas\n",
    "\n",
    "Traemos aquí nuestro preprocesado por columnas pero ahora sin incluir las variables con *lags*. De las variables *lags* se encarga la  clase `ForecasterRecursive`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = baseDiaria.asfreq('D')\n",
    "# X['Jornada'] = pd.Categorical(X['Jornada']).codes\n",
    "X_test = X[-steps:]\n",
    "X = X[:-steps]\n",
    "# Operations by columns\n",
    "# La columna Dtotal ahora se maneja dentro del Forecaster\n",
    "num_attribs = ['Tmedia']\n",
    "cat_attribs = ['Jornada']\n",
    "pass_attribs = ['Interval']\n",
    "\n",
    "preprocessor_exog = ColumnTransformer([\n",
    "    (\"num\", PolynomialFeatures(), num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    (\"pass\", 'passthrough', pass_attribs),\n",
    "    # Default drop para columnas no especificadas\n",
    "],remainder='passthrough', verbose_feature_names_out=False)\n",
    "\n",
    "preprocessor_exog.set_params(num__degree=2,\n",
    "                            num__include_bias=False,\n",
    "                            cat__drop='first')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "Aquí definimos `forecaster`. Vemos que necesita especifcar por separado al preprocesador de las variables exógenas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtesting forecaster\n",
    "# ==============================================================================\n",
    "\n",
    "forecaster = ForecasterRecursive(estimator = LinearRegression(),\n",
    "                                 lags = 7,\n",
    "                                 transformer_exog = preprocessor_exog\n",
    "                                )\n",
    "forecaster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "El método `create_train_X_y` nos permite inspeccionar cómo lucen los atributos luego de agregar los *lags* y el preprocesado de las variables exógeneas, justo antes de entrar al estimador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect training matrices\n",
    "# ==============================================================================\n",
    "X_train, y_train = forecaster.create_train_X_y(\n",
    "                       y    = X['Dtotal'],\n",
    "                       exog = X[['Tmedia', 'Jornada', 'Interval']]\n",
    "                   )\n",
    "X_train.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "En lo siguiente, definimos un esquema de validación con reajuste (*refit*) intermitente. Cada bloque de entrenamiento contiene un 20% más de los datos. Definimos `initial_train_size` como la cantidad de pasos igual al 20% de los datos. Como solo pronosticamos un paso adelante (`steps=1`) y re-entrenamos cada `initial_train_size` pasos (steps), esto conduce a un esquema de *backtesting* con 20%, 40%, 60% y 80% de datos de entrenamiento, sucesivamente.\n",
    "\n",
    "El esquema de *backtesting* descrito en el párrafo anterior se emplea en la búsqueda de mejores hiper-parámetros mediante la función ~ grid_search_forecaster`. Aquí, también debemos especificar por separado las variables endógenas y exógenas. Se observa que se puede pasar un lista de *lags* para evaluar en la grilla. Asimismo, admite un diccionario con lista de parámetros del estimador. En principio, no parece funcionar el diccionario con la lista de hiper_parámetros para el preprocesador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_train_size = int(0.2 * len(X['Dtotal']))\n",
    "end_train = X.index[initial_train_size]\n",
    "cv = TimeSeriesFold(\n",
    "        steps                 = 1,\n",
    "        initial_train_size    = end_train,\n",
    "        refit                 = initial_train_size,\n",
    "        fixed_train_size      = False,\n",
    "        gap                   = 0,\n",
    "        allow_incomplete_fold = True\n",
    "     )\n",
    "\n",
    "# Lags used as predictors\n",
    "lags_grid = [6,7,8]\n",
    "\n",
    "# Estimator hyperparameters\n",
    "# param_grid = {'preprocessor_exog__num__degree': np.arange(2,11).tolist()}\n",
    "param_grid = {'fit_intercept': [True]}\n",
    "\n",
    "df_results=pd.DataFrame([])\n",
    "for i, degree in enumerate([4,5,6,7]):\n",
    "\n",
    "    print(f\"Grid search for degree: {degree}\")\n",
    "    print(\"-------------------------\")\n",
    "    \n",
    "    preprocessor_exog.set_params(num__degree=degree,\n",
    "                            num__include_bias=False,\n",
    "                            cat__drop='first')\n",
    "\n",
    "\n",
    "    results = grid_search_forecaster(forecaster = forecaster,\n",
    "                                     y = X['Dtotal'],\n",
    "                                     cv = cv,\n",
    "                                     param_grid = param_grid,\n",
    "                                     metric = 'mean_squared_error',\n",
    "                                     exog = X[['Tmedia', 'Jornada', 'Interval']],\n",
    "                                     lags_grid = lags_grid,\n",
    "                                     return_best = False,\n",
    "                                     n_jobs = 'auto',\n",
    "                                     verbose = False,\n",
    "                                     show_progress = True\n",
    "          )\n",
    "    results['degree']=degree\n",
    "    df_results = pd.concat([df_results, results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['RMSE'] = np.sqrt(df_results['mean_squared_error'])\n",
    "df_results.drop(columns=['lags_label', 'params'], inplace=True)\n",
    "df_results.sort_values(by='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "Vemos que, nuevamente, el mejor ajuste se logra con un polinomio de grado 6. El RMSE es similar a los obtenidos anteriorimente. Entendemos que aquí es algo más bajo (mejor) para igual grado y número de lags ya que no se realice imputación con la media..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "#### Final model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train forecaster with the best hyperparameters and lags found\n",
    "# ==============================================================================\n",
    "preprocessor_exog.set_params(num__degree=6,\n",
    "                            num__include_bias=False,\n",
    "                            cat__drop='first')\n",
    "forecaster.set_lags(8)\n",
    "forecaster.fit(y=X['Dtotal'], exog=X[['Tmedia', 'Jornada', 'Interval']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "# ==============================================================================\n",
    "predictions = forecaster.predict(steps=steps, exog=X_test[['Tmedia', 'Jornada', 'Interval']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions versus test data\n",
    "# ==============================================================================\n",
    "fig, ax = plt.subplots(figsize=(6, 2.5))\n",
    "X_test['Dtotal'].plot(ax=ax, label='test')\n",
    "predictions.plot(ax=ax, label='predictions')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "## Parte C - Ejercicios\n",
    "\n",
    "1. Realizar el ajuste de hiper-parámetros variando los *lags* entre 5 y 15, y el grado de los atributos polinómicos entre 2 y 8. Emplear el método `grid_search_forecaster` con un esquema de validación por *backtesting* sin reajuste (*without refit*) para reducir el tiempo de búsqueda. Obtener el modelo final con los mejores parámetros. Evaluar por backtesting a 14 días con reajuste de parámetros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
